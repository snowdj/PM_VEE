# Local Interpretable Model-agnostic Explanations (LIME) {#LIME}

## Introduction {#LIMEIntroduction}

Ceteris-paribus (CP) profiles, introduced in Chapter \@ref(ceterisParibus), are suitable for models with a small number of interpretable explanatory variables. In case of such models it makes sense to explore each variable separately and analyze how does it affect model predictions.

Break-down (BD) plots and plots of Shapley values, introduced in Chapters \@ref(breakDown) and \@ref(shapley), respectively, are most suitable for modela with a small or moderate number of explanator variables. These plots do not offer as detailed information as the CP profiles, but can include more variables.

None of those approaches is well-suited for models with a large number of explanatory variables. Such models with even thousands of variables are not uncommon in, for instance, genomics. Also, if most of explanatory variables are binary, then CP profiles and BD plots are not very infromative. In such cases, sparse explainers offer a useful alternative. The most popular example of such explainers are Local Interpretable Model-agnostic Explanations (LIME) and their modifications.

The LIME method was originally proposed in [@lime]. The key idea behind this method is to locally approximate a black-box model by a simpler white-box model, which is easier to interpret. In this chapter, we describe the approach.  

## Intuition {#LIMEIntuition}

The intuition behind LIME is explained in Figure \@ref(fig:limeEx). We want to understand the predictions of a complex black-box model around a single instance of interest. The model presented in Figure \@ref(fig:limeEx) is a binary classifier, i.e., it pertains to a binary dependent variable. The axes represent the values of two continuous explanatory variables. The colored areas correspond to the decision regions, i.e., they indicate for which combinations of the variables the model classifies the observation to one of the two classes. The instance of interest is marked with the large black dot. By using an artificial dataset around the instance of interest, we can use a simpler white-box model that will locally approximate the predictions of the black-box model. The white-box model may then serve as a ''local explainer'' for the more complex model.

We may select different classes of white-box models. The most typical choices are regularized linear models like LASSO regression or decision trees [TOMASZ: ADD REFERENCES]. The important point is to limit the complexity of the models, so that they are easier to explain. 

```{r limeEx, echo=FALSE, fig.cap="(fig:limeEx) The idea behind local model approximations. The axes represent the values of two continuous explanatory variables in a binary-classification mode. The colored areas for which combinations of the variables the model classifies the observation to one of the two classes. To ''explain'' the prediction for the instance of interest (the large black dot), an artificial dataset around it is used to construct a simpler white-box model (here, a logistic-regression model, indicated by the dashed line) that locally approximates the predictions of the black-box model.", out.width = '70%', fig.align='center'}
knitr::include_graphics("figure/limeEx.png")
```

## Method {#LIMEMethod}

We want to find a model that locally approximates a black-box model $f()$ around the instance of interest $x_*$. Consider class $G$ of interpretable models. To find the required approximation, We can can consider the following ''loss function,''

$$
L(f, g, \Pi_{x_*}) + \Omega (g), 
$$
where model $g()$ belongs to class $G$, $\Pi_{x_*}$ of $x^*$ defines a neighborhood of $x_*$ in which approximation is sought, $L()$ is a goodness-of-fit measure (e.g., the likelihood), and $\Omega(g)$ is a penalty for the complexity of model $g()$. The penalty is used to select simple models from class $G$. 

Note that the models $f()$ and $g()$ may apply to different data spaces. The black-box model (function) $f(x):\mathcal X \rightarrow \mathcal R$ is defined on the original, lare-dimensional space $\mathcal X$. The white-box model (function) $g:\mathcal X' \rightarrow \mathcal R$ applies to a lower-dmension, more interpretable space $\mathcal X'$. We will present some examples of $\mathcal X'$ in the next section. For now we will just assume that function $h()$ transforms $\mathcal X$ into $\mathcal X'$.

If we limit class $G$ to sparse linear models, the following algorithm may be used to find an interpretable white-box model $g()$ that includes $K$ most important, interpretable explanatory variables: 

```
Input: N - sample size for the white-box model [TOMASZ: CHANGED. CORRECT?]
Input: K - number of variables for the white-box model [TOMASZ: CHANGED. CORRECT?]
1. Let x' = h(x) be an interpretable version of x  [TOMASZ: WHAT IS x?]
2. for i in 1...N {
3.   z'[i] <- sample_around(x') 
4.   y'[i] <- f(z[i]) # prediction for a new observation [TOMASZ: WHAT IS z[i]?]
5.   w'[i] <- similarity(x, z[i]) [TOMASZ: WHAT IS SIMILARITY? z[i]?]
6. }
7. return K-LASSO(y', x', w')
```

In Step 7, $K-LASSO(y', x', w')$ stands for a weighted LASSO linear-regression that selects $K$ most important variables based on new dataset $(y', x')$ with weights $w'$. 

The practical implementation of the idea involves three important steps, which are discussed in the subsequent sub-sections.

### Interpretable data representation

As it has been mentioned, the black-box model $f()$ and the white-box model $g()$ may apply to different data spaces. For example, let's consider a VGG16 neural network applied to ImageNet data. [TOMASZ: IS IT A WELL-KNOWN EXAMPLE? ANY REFERENCE?] The model uses an image of the size of $244 \times 244$ pixels as input and predicts to which of 1000 potential categories does the image belong to. The original data space is of dimension $3 \times 244 \times 244$, i.e., it is 178,608-dimensional with three single-color channels (red, green, blue) for a single pixel. Explaining predictions in such a high-dimensional space is difficult. Instead, the space can be transformed into superpixels, which are treated as binary features that can be turned on or off. Figure \@ref(fig:duckHorse06) presents an example of 100 superpixels created for an ambiguous picture. Thus, in this case the black-box model $f()$ operates in principle on data space $\mathcal X=R^{178608}$ [TOMASZ: NOT SURE ABOUT THAT, COLOR COORDINATES ARE PROBABLY 256 VALUES OR SOMETHING LIKE THAT.], while the white-box model $g()$ works on space $\mathcal X' = \{0,1\}^{100}$.

It is worth noting that superpixels are frequent choices for image data. For text data, words are frequently used as interpretable variables. To reduce to complexity of the data space, continuous variables are often discretized to obtain interpretable tabular data. In case of categorical variables, combination of categires is often used.

```{r duckHorse06, echo=FALSE, fig.cap="(fig:duckHorse06) The left panel shows an ambiguous picture, half-horse and half-duck. The right panel shows 100 superpixels identified for this figure. [TOMASZ: SOURCE OF THE IMAGE?]", out.width = '100%', fig.align='center'}
knitr::include_graphics("figure/duck_horse_06.png")
```

### Sampling around the instance of interest

To develop the locally-approximation white-box model, new data points around the instance of interest are needed. It may not be enough to sample points from the original dataset, because in a high-dimensional data space the data are usually very sparse and data points are ''far'' from each other. For this reason, the data for the development of the white-box model are often created by using perturbations of the instance of interest. 

For a set of binary variables, the common choice is to change (from 0 to 1 or from 1 to 0) the value of a randomly-selected number of variables describing the instance of interest. 

For continuous variables, various proposals are introduced in different papers. [TOMASZ: WE SHOULD PROVIDE OME CONCRETE IDEAS WITH REFERENCES.]

In the example of the duck-horse in Figure \@ref(fig:duckHorse06), the perturbations of the image would be created by randomly including or excluding some of the superpixels.

### Developing the white-box model

Once the new data were sampled around the instance of interest, we may attempt to develop an interpretable white-box model $g()$ from class $G$. 

The most common choices for $G$ are generalized linear models. To get sparse models, i.e., models with a limited number of variables, LASSO or similar regularization-modelling techniques are used. For instance, in the algorithm presented in Section \@ref(LIMEMethod), the $K-LASSO$ method has been mentioned. An alternative choice are classification-and-regression trees.

Figure \@ref(fig:duckHorse04) presents LIME for the top two classes [TOMASZ: WHAT DOES IT MEAN?] selected by the neural network VGG16. The explanations were obtained with the $K-LASSO$ method which selected $K$ [TOMASZ: WHAT WAS THE VALUE OF K?] superpixels that were the most influential from the model-prediction point of view. For each of the two classes, the top $K$ [TOMASZ: WHAT WAS THE VALUE OF K?] superpixels are highlighted. It is interesting to observe that the superpixel which contains the beak is influential for the prediction ''goose,'' while the superpixels linked with the colour are influential for the prediction ''standard poodle''.

```{r duckHorse04, echo=FALSE, fig.cap="(fig:duckHorse04) LIME for two predictions (''standard poodle'' and ''goose'') obtained by the VGG16 network with ImageNet weights for the half-duck, half-horse image. [TOMASZ: SOURCE OF THE IMAGE?]", out.width = '100%', fig.align='center'}
knitr::include_graphics("figure/duck_horse_04.png")
```

## Example: Titanic data {#LIMEExample}

Let us consider the random-forest model `titanic_rf_v6` (see Section \@ref(model-titanic-rf) and passenger `johny_d` (see Section \@ref(predictions-titanic)) as the instance of interest in the Titanic data. 

Figure \@ref(fig:LIMEexample01) presents explanations generated by the LIME method.[TOMASZ: WHAT ARE THOSE NUMBERS? WHAT WAS THE SIMPLIFIED MODEL? LOGISITIC REGRESSION? ANY PENALTY FOR COMPLEXITY?] Three variables are identified as the most influential: age, gender, and class. Note that, for age, a dichotomized version of the orignially conitinuous variable is used. On the other hand, for class, a dichotomized version based on the combination of several original categories is used. 


```{r LIMEexample01, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="(fig:LIMEexample01) LIME method for the prediction for `johny_d` for the random-forest model `titanic_rf_v6` and the Titanic data. [TOMASZ: WHAT ARE THOSE NUMBERS?]", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/LIMEexample01.png")
```

Figure \@ref(fig:LIMEexample02) illustrates how the two levels for age can be extracted from the CP. [TOMASZ: HOW ARE THE TWO LEVELS OBTAINED?] 

```{r LIMEexample02, warning=FALSE, message=FALSE, echo=FALSE, fig.cap="(fig:LIMEexample02) Interpretable variable generated for age. LIME method for the prediction for `johny_d` for the random-forest model `titanic_rf_v6` and the Titanic data.", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/LIMEexample02.png")
```


```{r, warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE}
load("models/explain_rf_v6.rda")
load("models/titanic.rda")
load("models/henry.rda")
library("localModel")
library("DALEX")
library("ggplot2")
library("randomForest")

localModel_lok <- individual_surrogate_model(explain_rf_v6, johny_d,
                                        size = 5000, seed = 1313)
localModel_lok
plot(localModel_lok) + facet_null() + ggtitle("localModel explanations for Johny D","Random Forest v6") + theme_drwhy_vertical()
plot_interpretable_feature(localModel_lok, "age") + ggtitle("Interpretable representation for age","Random Forest v6" ) + xlab("age") + ylab("model response")

```

## Pros and cons {#LIMEProsCons}

As mentioned by [@lime], the LIME method 
- is *model-agnostic*, as it does not imply any assumptions on the black-box model structure, 
- offers an *interpretable representation*, because the original data space is transformed into a more interpretable lower-dimension space (like tranformation from individual pixels to super pixels for image data),
- provides *local fidelity*, i.e., the explanations are locally well-fitted to the black-box model.

The method has been widely adopted in text and image analysis, in part due to the interpretable data representation. The underlying intuition for the method is easy to understand: a simpler model is used to approximate a more complex one. By using a simpler model, with a smaller number of interpretable explanatory variabes, predictions are easier to explain. The LIME method can be applied to complex, high-dimensional models.

There are several important limitations. For instance, despite several proposals, the issue of finding interpretable representations for continuous and categorical variables is not solved yet. Also, because the white-box model is selected to approximate the black-box model, and the data themselves, the method does not control the quality of the local fit of the white-box model to the data. Thus, the latter model may be misleading.

Finally, in high-dimensional data, data points are sparse. Defining a ''local neighborhood'' of the instance of interest may not be straghtforward.


## Code snippets for R {#LIMERcode}

LIME and similar methods are implemented in various R and Python packages. For example, `lime` [@R-lime] is a port of the LIME Python library [@shapPackage], while `live` [@R-live], `localModel` [@localModelPackage], and `iml` [@imlRPackage] are separate R packages. 

Different implementations of LIME offer different algorithms for extraction of interpretable features, different methods for sampling, and different methods of weighting. For instance, regarding transformation of continuous variables into interpretable features, `lime` performs global discretization using quartiles, `localModel` performs local discretization using CP profiles, while `live` and `iml` work directly on continuous variables.

Due to these differences, the packages yield different results (explanations).

In what follows, for illustration purposes, we use the `titanic_rf_v6` random-forest model for the Titanic data developed in Section \@ref(model-titanic-rf). Recall that it is developed to predict the probability of survival from sinking of Titanic. Instance-level explanations are calculated for a single observation: `johny_d` - an 8-year-old passenger that travelled in the 1st class. `DALEX` explainers for the model and the `jonhy_d` data are retrieved via `archivist` hooks as listed in Section \@ref(ListOfModelsTitanic). 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("DALEX")
library("randomForest")

titanic <- archivist::aread("pbiecek/models/27e5c")
titanic_rf_v6 <- archivist::aread("pbiecek/models/31570")
johny_d <- archivist::aread("pbiecek/models/e3596")
```

### The lime package

The key elements of the `lime` package are functions `lime()`, which creates an explainer, and `explain()`, which evaluates explanations.

The detailed results for the `titanic_rf_v6` random-forest model and `johny_d` are presented below. [TOMASZ: WHAT DOES THE OUTPUT INCLUDE?]  The implemented version of LIME dichotomizes continuous variables by using quartiles. Hence, in the output we get a binary variable `age < 22`. [TOMASZ: WHAT ABOUT THE FORM OF THE WHITE-BOX MODEL?] 

The graphical presentation of the results, obtained by applying the generic `plot()` function to the object resulting from the `application of the `explain()` function, is provided in Figure \@ref(fig:limeExplLIMETitanic). [TOMASZ: WHAT IS PRESENTED THERE? WHY THERE IS A CHANGE OF SIGN IN THE PLOT AS COMPARED TO THE PRINTOUT?] 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("lime")
model_type.randomForest <- function(x, ...) "classification"
lime_rf <- lime(titanic[,colnames(johny_d)], titanic_rf_v6)
lime_expl <- lime::explain(johny_d, lime_rf, labels = "yes", n_features = 4, n_permutations = 1000)
lime_expl

#      model_type case label label_prob  model_r2 model_intercept model_prediction
#1 classification    1    no      0.602 0.5806297       0.5365448        0.5805939
#2 classification    1    no      0.602 0.5806297       0.5365448        0.5805939
#3 classification    1    no      0.602 0.5806297       0.5365448        0.5805939
#4 classification    1    no      0.602 0.5806297       0.5365448        0.5805939
#  feature feature_value feature_weight  feature_desc                 data   prediction
#1    fare            72     0.00640936  21.00 < fare 1, 2, 8, 0, 0, 72, 4 0.602, 0.398
#2  gender             2     0.30481181 gender = male 1, 2, 8, 0, 0, 72, 4 0.602, 0.398
#3   class             1    -0.16690730   class = 1st 1, 2, 8, 0, 0, 72, 4 0.602, 0.398
#4     age             8    -0.10026475     age <= 22 1, 2, 8, 0, 0, 72, 4 0.602, 0.398

plot_features(lime_expl)
```

```{r limeExplLIMETitanic, echo=FALSE, fig.cap="(fig:limeExplLIMETitanic) LIME-method results for the prediction for `johny_d` for the random-forest model `titanic_rf_v6` and the Titanic data, generated by the `lime` package. ", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/lime_expl_lime_titanic.png")
```

### The localModel package

The key elements of the `localModel` package are functions ``DALEX::explain()``, which creates an explainer, and `individual_surrogate_model()`, which develops the local white-box model.

The detailed results for the `titanic_rf_v6` random-forest model and `johny_d` are presented below. [TOMASZ: WHAT DOES THE OUTPUT INCLUDE?]  [TOMASZ: WHAT ABOUT THE FORM OF THE WHITE-BOX MODEL?] The implemented version of LIME dichotomizes continuous variables by using CP profiles. The CP profile for `johny_d`, presented in Figure \@ref(fig:titanicCeterisProfile01D) in Chapter \@ref(ceterisParibus), indicated that, for age, the largest drop in the predicted probability of survival was observed for the age increasing beyond 15 years. Hence, in the output of the `individual_surrogate_model()`, we see a binary variable `age < 15.36`.   

The graphical presentation of the results, obtained by applying the generic `plot()` function to the object resulting from the `application of the `explain()` function, is provided in Figure \@ref(fig:limeExplLocalModelTitanic). [TOMASZ: WHAT IS PRESENTED THERE? ] 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("localModel")

localModel_rf <- DALEX::explain(model = titanic_rf_v6,
                     data = titanic[,colnames(johny_d)])
localModel_lok <- individual_surrogate_model(localModel_rf, johny_d,
                                        size = 1000, seed = 1313)
localModel_lok
#   estimated                    variable dev_ratio response
#1 0.23479837                (Model mean) 0.6521442         
#2 0.14483341                 (Intercept) 0.6521442         
#3 0.08081853 class = 1st, 2nd, deck crew 0.6521442         
#4 0.00000000     gender = female, NA, NA 0.6521442         
#5 0.23282293                age <= 15.36 0.6521442         
#6 0.02338929                fare > 31.05 0.6521442    
plot(localModel_lok)
```


```{r limeExplLocalModelTitanic, echo=FALSE, fig.cap="(fig:limeExplLocalModelTitanic) LIME-method results for the prediction for `johny_d` for the random-forest model `titanic_rf_v6` and the Titanic data, generated by the `localModel` package. ", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/lime_expl_localModel_titanic.png")
```

### The iml package

The key elements of the `iml` package are functions ``Predictor$new()``, which creates an explainer, and `LocalModel$new()`, which develops the local white-box model.

The detailed results for the `titanic_rf_v6` random-forest model and `johny_d` are presented below. [TOMASZ: WHAT DOES THE OUTPUT INCLUDE?]  [TOMASZ: WHAT ABOUT THE FORM OF THE WHITE-BOX MODEL?] The implemented version of LIME does not transform continuous variables. The CP profile for `johny_d`, presented in Figure \@ref(fig:titanicCeterisProfile01D) in Chapter \@ref(ceterisParibus), indicated that, for boys younger than 15-year-old, the predicted probability of survival did not change very much. Hence, in the printed output, age does not appear as an important variable.  [TOMASZ: JOHNY_D EMBARKED IN BELFAST, NOT SOUTHAMPTON.]

The graphical presentation of the results, obtained by applying the generic `plot()` function to the object resulting from the `application of the `explain()` function, is provided in Figure \@ref(fig:limeExplIMLTitanic). [TOMASZ: WHAT IS PRESENTED THERE? WHY IS THERE AGE, THOUGH IT IS ABSENT IN THE PRINTOUT?] 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("iml")
iml_rf = Predictor$new(titanic_rf_v6, data = titanic[,colnames(johny_d)])
iml_glass_box = LocalModel$new(iml_rf, x.interest = johny_d, k = 6)
iml_glass_box
#Interpretation method:  LocalModel 
#
#Analysed predictor: 
#Prediction task: unknown 
#
#Analysed data:
#Sampling from data.frame with 2207 rows and 7 columns.
#
#Head of results:
#          beta x.recoded     effect  x.original              feature
#1 -0.158368701         1 -0.1583687         1st            class=1st
#2  1.739826204         1  1.7398262        male          gender=male
#3  0.018515945         0  0.0000000           0                sibsp
#4 -0.001484918        72 -0.1069141          72                 fare
#5  0.131819869         1  0.1318199 Southampton embarked=Southampton
#6  0.158368701         1  0.1583687         1st            class=1st

plot(iml_glass_box) 
```

```{r limeExplIMLTitanic, echo=FALSE, fig.cap="(fig:limeExplIMLTitanic) LIME-method results for the prediction for `johny_d` for the random-forest model `titanic_rf_v6` and the Titanic data, generated by the `iml` package. ", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/lime_expl_iml_titanic.png")
```


