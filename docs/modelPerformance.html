<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Model Performance Measures | Predictive Models: Explore, Explain, and Debug</title>
  <meta name="description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Model Performance Measures | Predictive Models: Explore, Explain, and Debug" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  <meta name="github-repo" content="pbiecek/PM_VEE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Model Performance Measures | Predictive Models: Explore, Explain, and Debug" />
  
  <meta name="twitter:description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski" />


<meta name="date" content="2019-12-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelLevelExploration.html"/>
<link rel="next" href="featureImportance.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-5650686-15', 'https://pbiecek.github.io/PM_VEE/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>
<style>
.figure {
   padding:40px 0px;
}
</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Predictive Models: Explore, Explain, and Debug<br/> Human-Centered Interpretable Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#notes-to-readers"><i class="fa fa-check"></i><b>1.1</b> Notes to readers</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.2</b> The aim of the book</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#three-single-laws"><i class="fa fa-check"></i><b>1.3</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#bookstructure"><i class="fa fa-check"></i><b>1.4</b> The structure of the book</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#terminology"><i class="fa fa-check"></i><b>1.5</b> Terminology</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#glass-box-models-vs.black-box-models"><i class="fa fa-check"></i><b>1.6</b> Glass-box models vs. black-box models</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#model-agnostic-vs.model-specific-approach"><i class="fa fa-check"></i><b>1.7</b> Model-agnostic vs. model-specific approach</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#what-is-in-this-book-and-what-is-not"><i class="fa fa-check"></i><b>1.8</b> What is in this book and what is not</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#thanksto"><i class="fa fa-check"></i><b>1.9</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html"><i class="fa fa-check"></i><b>2</b> Model Development</a><ul>
<li class="chapter" data-level="2.1" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation - from introduction</a></li>
<li class="chapter" data-level="2.2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#model-visualization-exploration-and-explanation---from-introduction"><i class="fa fa-check"></i><b>2.2</b> Model visualization, exploration, and explanation - from introduction</a></li>
<li class="chapter" data-level="2.3" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPIntro"><i class="fa fa-check"></i><b>2.3</b> Introduction</a></li>
<li class="chapter" data-level="2.4" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#the-process"><i class="fa fa-check"></i><b>2.4</b> The Process</a></li>
<li class="chapter" data-level="2.5" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#data-preparation"><i class="fa fa-check"></i><b>2.5</b> Data preparation</a></li>
<li class="chapter" data-level="2.6" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#data-exploration"><i class="fa fa-check"></i><b>2.6</b> Data exploration</a></li>
<li class="chapter" data-level="2.7" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#model-assembly"><i class="fa fa-check"></i><b>2.7</b> Model assembly</a></li>
<li class="chapter" data-level="2.8" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#model-understanding"><i class="fa fa-check"></i><b>2.8</b> Model understanding</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html"><i class="fa fa-check"></i><b>3</b> Do-it-yourself With R</a><ul>
<li class="chapter" data-level="3.1" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#what-to-install"><i class="fa fa-check"></i><b>3.1</b> What to install?</a></li>
<li class="chapter" data-level="3.2" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#how-to-work-with-dalex"><i class="fa fa-check"></i><b>3.2</b> How to work with <code>DALEX</code>?</a></li>
<li class="chapter" data-level="3.3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#how-to-work-with-archivist"><i class="fa fa-check"></i><b>3.3</b> How to work with <code>archivist</code>?</a></li>
<li class="chapter" data-level="3.4" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#Packages"><i class="fa fa-check"></i><b>3.4</b> DrWhy Packages</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html"><i class="fa fa-check"></i><b>4</b> Do-it-yourself With Python</a></li>
<li class="chapter" data-level="5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html"><i class="fa fa-check"></i><b>5</b> Data sets and models</a><ul>
<li class="chapter" data-level="5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>5.1</b> Sinking of the RMS Titanic</a><ul>
<li class="chapter" data-level="5.1.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-titanic"><i class="fa fa-check"></i><b>5.1.1</b> Data exploration</a></li>
<li class="chapter" data-level="5.1.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-lmr"><i class="fa fa-check"></i><b>5.1.2</b> Logistic regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-rf"><i class="fa fa-check"></i><b>5.1.3</b> Random forest</a></li>
<li class="chapter" data-level="5.1.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-gbm"><i class="fa fa-check"></i><b>5.1.4</b> Gradient boosting</a></li>
<li class="chapter" data-level="5.1.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic"><i class="fa fa-check"></i><b>5.1.5</b> Model predictions</a></li>
<li class="chapter" data-level="5.1.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicRCode"><i class="fa fa-check"></i><b>5.1.6</b> Explainers</a></li>
<li class="chapter" data-level="5.1.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsTitanic"><i class="fa fa-check"></i><b>5.1.7</b> List of objects for the <code>titanic</code> example</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>5.2</b> Apartment prices</a><ul>
<li class="chapter" data-level="5.2.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-apartments"><i class="fa fa-check"></i><b>5.2.1</b> Data exploration</a></li>
<li class="chapter" data-level="5.2.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-lr"><i class="fa fa-check"></i><b>5.2.2</b> Linear regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-rf"><i class="fa fa-check"></i><b>5.2.3</b> Random forest</a></li>
<li class="chapter" data-level="5.2.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictionsApartments"><i class="fa fa-check"></i><b>5.2.4</b> Model predictions</a></li>
<li class="chapter" data-level="5.2.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsRCode"><i class="fa fa-check"></i><b>5.2.5</b> Explainers</a></li>
<li class="chapter" data-level="5.2.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsApartments"><i class="fa fa-check"></i><b>5.2.6</b> List of objects for the <code>apartments</code> example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="instance-level.html"><a href="instance-level.html"><i class="fa fa-check"></i>Instance Level</a></li>
<li class="chapter" data-level="6" data-path="InstanceLevelExploration.html"><a href="InstanceLevelExploration.html"><i class="fa fa-check"></i><b>6</b> Introduction to Instance Level Exploration</a></li>
<li class="chapter" data-level="7" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>7</b> Break-down Plots for Additive Variable Attributions</a><ul>
<li class="chapter" data-level="7.1" data-path="breakDown.html"><a href="breakDown.html#BDIntuition"><i class="fa fa-check"></i><b>7.1</b> Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="breakDown.html"><a href="breakDown.html#BDMethod"><i class="fa fa-check"></i><b>7.2</b> Method</a><ul>
<li class="chapter" data-level="7.2.1" data-path="breakDown.html"><a href="breakDown.html#break-down-for-linear-models"><i class="fa fa-check"></i><b>7.2.1</b> Break-down for linear models</a></li>
<li class="chapter" data-level="7.2.2" data-path="breakDown.html"><a href="breakDown.html#break-down-for-general-case"><i class="fa fa-check"></i><b>7.2.2</b> Break-down for general case</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="breakDown.html"><a href="breakDown.html#BDExample"><i class="fa fa-check"></i><b>7.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="7.4" data-path="breakDown.html"><a href="breakDown.html#BDProsCons"><i class="fa fa-check"></i><b>7.4</b> Pros and cons</a></li>
<li class="chapter" data-level="7.5" data-path="breakDown.html"><a href="breakDown.html#BDR"><i class="fa fa-check"></i><b>7.5</b> Code snippets for R</a><ul>
<li class="chapter" data-level="7.5.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-break_down-function"><i class="fa fa-check"></i><b>7.5.1</b> Basic use of the <code>break_down()</code> function</a></li>
<li class="chapter" data-level="7.5.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-break_down-function"><i class="fa fa-check"></i><b>7.5.2</b> Advanced use of the <code>break_down()</code> function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="iBreakDown.html"><a href="iBreakDown.html"><i class="fa fa-check"></i><b>8</b> Break-down Plots for Models with Interactions (iBreak-down Plots)</a><ul>
<li class="chapter" data-level="8.1" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDIntuition"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDMethod"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDExample"><i class="fa fa-check"></i><b>8.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="8.4" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDProsCons"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDRcode"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9</b> Shapley Additive Explanations (SHAP) and Average Variable Attributions</a><ul>
<li class="chapter" data-level="9.1" data-path="shapley.html"><a href="shapley.html#SHAPIntuition"><i class="fa fa-check"></i><b>9.1</b> Intuition</a></li>
<li class="chapter" data-level="9.2" data-path="shapley.html"><a href="shapley.html#SHAPMethod"><i class="fa fa-check"></i><b>9.2</b> Method</a></li>
<li class="chapter" data-level="9.3" data-path="shapley.html"><a href="shapley.html#SHAPExample"><i class="fa fa-check"></i><b>9.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="9.4" data-path="shapley.html"><a href="shapley.html#SHAProsCons"><i class="fa fa-check"></i><b>9.4</b> Pros and cons</a></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html#SHAPRcode"><i class="fa fa-check"></i><b>9.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>10</b> Local Interpretable Model-agnostic Explanations (LIME)</a><ul>
<li class="chapter" data-level="10.1" data-path="LIME.html"><a href="LIME.html#LIMEIntroduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="LIME.html"><a href="LIME.html#LIMEIntuition"><i class="fa fa-check"></i><b>10.2</b> Intuition</a></li>
<li class="chapter" data-level="10.3" data-path="LIME.html"><a href="LIME.html#LIMEMethod"><i class="fa fa-check"></i><b>10.3</b> Method</a><ul>
<li class="chapter" data-level="10.3.1" data-path="LIME.html"><a href="LIME.html#interpretable-data-representation"><i class="fa fa-check"></i><b>10.3.1</b> Interpretable data representation</a></li>
<li class="chapter" data-level="10.3.2" data-path="LIME.html"><a href="LIME.html#sampling-around-the-instance-of-interest"><i class="fa fa-check"></i><b>10.3.2</b> Sampling around the instance of interest</a></li>
<li class="chapter" data-level="10.3.3" data-path="LIME.html"><a href="LIME.html#developing-the-white-box-model"><i class="fa fa-check"></i><b>10.3.3</b> Developing the white-box model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="LIME.html"><a href="LIME.html#LIMEExample"><i class="fa fa-check"></i><b>10.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="10.5" data-path="LIME.html"><a href="LIME.html#LIMEProsCons"><i class="fa fa-check"></i><b>10.5</b> Pros and cons</a></li>
<li class="chapter" data-level="10.6" data-path="LIME.html"><a href="LIME.html#LIMERcode"><i class="fa fa-check"></i><b>10.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="10.6.1" data-path="LIME.html"><a href="LIME.html#the-lime-package"><i class="fa fa-check"></i><b>10.6.1</b> The lime package</a></li>
<li class="chapter" data-level="10.6.2" data-path="LIME.html"><a href="LIME.html#the-localmodel-package"><i class="fa fa-check"></i><b>10.6.2</b> The localModel package</a></li>
<li class="chapter" data-level="10.6.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>10.6.3</b> The iml package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>11</b> Ceteris-paribus Profiles and What-If Analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntro"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntuition"><i class="fa fa-check"></i><b>11.2</b> Intuition</a></li>
<li class="chapter" data-level="11.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPMethod"><i class="fa fa-check"></i><b>11.3</b> Method</a></li>
<li class="chapter" data-level="11.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPExample"><i class="fa fa-check"></i><b>11.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="11.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPProsCons"><i class="fa fa-check"></i><b>11.5</b> Pros and cons</a></li>
<li class="chapter" data-level="11.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPR"><i class="fa fa-check"></i><b>11.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="11.6.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#basic-use-of-the-ceteris_paribus-function"><i class="fa fa-check"></i><b>11.6.1</b> Basic use of the <code>ceteris_paribus</code> function</a></li>
<li class="chapter" data-level="11.6.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#advanced-use-of-the-ceteris_paribus-function"><i class="fa fa-check"></i><b>11.6.2</b> Advanced use of the <code>ceteris_paribus</code> function</a></li>
<li class="chapter" data-level="11.6.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#champion-challenger-analysis"><i class="fa fa-check"></i><b>11.6.3</b> Champion-challenger analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html"><i class="fa fa-check"></i><b>12</b> Ceteris-paribus Oscillations and Local Variable-importance</a><ul>
<li class="chapter" data-level="12.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntuition"><i class="fa fa-check"></i><b>12.2</b> Intuition</a></li>
<li class="chapter" data-level="12.3" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscMethod"><i class="fa fa-check"></i><b>12.3</b> Method</a></li>
<li class="chapter" data-level="12.4" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscExample"><i class="fa fa-check"></i><b>12.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscProsCons"><i class="fa fa-check"></i><b>12.5</b> Pros and cons</a></li>
<li class="chapter" data-level="12.6" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscR"><i class="fa fa-check"></i><b>12.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="12.6.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#basic-use-of-the-calculate_oscillations-function"><i class="fa fa-check"></i><b>12.6.1</b> Basic use of the <code>calculate_oscillations</code> function</a></li>
<li class="chapter" data-level="12.6.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#advanced-use-of-the-calculate_oscillations-function"><i class="fa fa-check"></i><b>12.6.2</b> Advanced use of the <code>calculate_oscillations</code> function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="localDiagnostics.html"><a href="localDiagnostics.html"><i class="fa fa-check"></i><b>13</b> Local Diagnostics With Ceteris-paribus Profiles</a><ul>
<li class="chapter" data-level="13.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntro"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntuition"><i class="fa fa-check"></i><b>13.2</b> Intuition</a></li>
<li class="chapter" data-level="13.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagMethod"><i class="fa fa-check"></i><b>13.3</b> Method</a><ul>
<li class="chapter" data-level="13.3.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagNeighbors"><i class="fa fa-check"></i><b>13.3.1</b> Nearest neighbors</a></li>
<li class="chapter" data-level="13.3.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProfiles"><i class="fa fa-check"></i><b>13.3.2</b> Profiles for neighbors</a></li>
<li class="chapter" data-level="13.3.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagLFplot"><i class="fa fa-check"></i><b>13.3.3</b> Local-fidelity plot</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagExample"><i class="fa fa-check"></i><b>13.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="13.5" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProsCons"><i class="fa fa-check"></i><b>13.5</b> Pros and cons</a></li>
<li class="chapter" data-level="13.6" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagR"><i class="fa fa-check"></i><b>13.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html"><i class="fa fa-check"></i><b>14</b> Summary of Instance-level Explainers</a><ul>
<li class="chapter" data-level="14.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#number-of-explanatory-variables-in-the-model"><i class="fa fa-check"></i><b>14.1</b> Number of explanatory variables in the model</a><ul>
<li class="chapter" data-level="14.1.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#low-to-medium-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.1.1</b> Low to medium number of explanatory variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#medium-to-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.1.2</b> Medium to large number of explanatory variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#very-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.1.3</b> Very large number of explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#correlated-explanatory-variables"><i class="fa fa-check"></i><b>14.2</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="14.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#models-with-interactions"><i class="fa fa-check"></i><b>14.3</b> Models with interactions</a></li>
<li class="chapter" data-level="14.4" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#sparse-explanations"><i class="fa fa-check"></i><b>14.4</b> Sparse explanations</a></li>
<li class="chapter" data-level="14.5" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#additional-uses-of-model-exploration-and-explanation"><i class="fa fa-check"></i><b>14.5</b> Additional uses of model exploration and explanation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-level.html"><a href="model-level.html"><i class="fa fa-check"></i>Model Level</a></li>
<li class="chapter" data-level="15" data-path="modelLevelExploration.html"><a href="modelLevelExploration.html"><i class="fa fa-check"></i><b>15</b> Introduction to Model Level Exploration</a></li>
<li class="chapter" data-level="16" data-path="modelPerformance.html"><a href="modelPerformance.html"><i class="fa fa-check"></i><b>16</b> Model Performance Measures</a><ul>
<li class="chapter" data-level="16.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntro"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntuition"><i class="fa fa-check"></i><b>16.2</b> Intuition</a></li>
<li class="chapter" data-level="16.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethod"><i class="fa fa-check"></i><b>16.3</b> Method</a><ul>
<li class="chapter" data-level="16.3.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCont"><i class="fa fa-check"></i><b>16.3.1</b> Continuous dependent variable</a></li>
<li class="chapter" data-level="16.3.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodBin"><i class="fa fa-check"></i><b>16.3.2</b> Binary dependent variable</a></li>
<li class="chapter" data-level="16.3.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCateg"><i class="fa fa-check"></i><b>16.3.3</b> Categorical dependent variable</a></li>
<li class="chapter" data-level="16.3.4" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCount"><i class="fa fa-check"></i><b>16.3.4</b> Count dependent variable</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="modelPerformance.html"><a href="modelPerformance.html#example"><i class="fa fa-check"></i><b>16.4</b> Example</a><ul>
<li class="chapter" data-level="16.4.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceApartments"><i class="fa fa-check"></i><b>16.4.1</b> Apartments data</a></li>
<li class="chapter" data-level="16.4.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceTitanic"><i class="fa fa-check"></i><b>16.4.2</b> Titanic data</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceProsCons"><i class="fa fa-check"></i><b>16.5</b> Pros and cons</a></li>
<li class="chapter" data-level="16.6" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceR"><i class="fa fa-check"></i><b>16.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="featureImportance.html"><a href="featureImportance.html"><i class="fa fa-check"></i><b>17</b> Variable’s Importance</a><ul>
<li class="chapter" data-level="17.1" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntro"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntuition"><i class="fa fa-check"></i><b>17.2</b> Intuition</a></li>
<li class="chapter" data-level="17.3" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceMethod"><i class="fa fa-check"></i><b>17.3</b> Method</a></li>
<li class="chapter" data-level="17.4" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceTitanic"><i class="fa fa-check"></i><b>17.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="17.5" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceProsCons"><i class="fa fa-check"></i><b>17.5</b> Pros and cons</a></li>
<li class="chapter" data-level="17.6" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceR"><i class="fa fa-check"></i><b>17.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="17.7" data-path="featureImportance.html"><a href="featureImportance.html#more-models"><i class="fa fa-check"></i><b>17.7</b> More models</a></li>
<li class="chapter" data-level="17.8" data-path="featureImportance.html"><a href="featureImportance.html#level-frequency"><i class="fa fa-check"></i><b>17.8</b> Level frequency</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>18</b> Partial Dependency Profiles</a><ul>
<li class="chapter" data-level="18.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntro"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntuition"><i class="fa fa-check"></i><b>18.2</b> Intuition</a></li>
<li class="chapter" data-level="18.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPMethod"><i class="fa fa-check"></i><b>18.3</b> Method</a><ul>
<li class="chapter" data-level="18.3.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependency-profiles"><i class="fa fa-check"></i><b>18.3.1</b> Partial Dependency Profiles</a></li>
<li class="chapter" data-level="18.3.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependency-profiles"><i class="fa fa-check"></i><b>18.3.2</b> Clustered Partial Dependency Profiles</a></li>
<li class="chapter" data-level="18.3.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependency-profiles"><i class="fa fa-check"></i><b>18.3.3</b> Grouped Partial Dependency Profiles</a></li>
<li class="chapter" data-level="18.3.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependency-profiles"><i class="fa fa-check"></i><b>18.3.4</b> Contrastive Partial Dependency profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPExample"><i class="fa fa-check"></i><b>18.4</b> Example: Apartments data</a><ul>
<li class="chapter" data-level="18.4.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependency-profiles-1"><i class="fa fa-check"></i><b>18.4.1</b> Partial Dependency Profiles</a></li>
<li class="chapter" data-level="18.4.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependency-profiles-1"><i class="fa fa-check"></i><b>18.4.2</b> Clustered Partial Dependency Profiles</a></li>
<li class="chapter" data-level="18.4.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependency-profiles-1"><i class="fa fa-check"></i><b>18.4.3</b> Grouped Partial Dependency Profiles</a></li>
<li class="chapter" data-level="18.4.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependency-profiles-1"><i class="fa fa-check"></i><b>18.4.4</b> Contrastive Partial Dependency profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPProsCons"><i class="fa fa-check"></i><b>18.5</b> Pros and cons</a></li>
<li class="chapter" data-level="18.6" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPR"><i class="fa fa-check"></i><b>18.6</b> Code snippets for R</a><ul>
<li class="chapter" data-level="18.6.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependency-profiles-2"><i class="fa fa-check"></i><b>18.6.1</b> Clustered Partial Dependency profiles</a></li>
<li class="chapter" data-level="18.6.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependency-profiles-2"><i class="fa fa-check"></i><b>18.6.2</b> Grouped Partial Dependency profiles</a></li>
<li class="chapter" data-level="18.6.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependency-profiles-2"><i class="fa fa-check"></i><b>18.6.3</b> Contrastive Partial Dependency profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>19</b> Accumulated Local Profiles</a><ul>
<li class="chapter" data-level="19.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntro"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntuition"><i class="fa fa-check"></i><b>19.2</b> Intuition</a></li>
<li class="chapter" data-level="19.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPMethod"><i class="fa fa-check"></i><b>19.3</b> Method</a><ul>
<li class="chapter" data-level="19.3.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#partial-dependency-profile"><i class="fa fa-check"></i><b>19.3.1</b> Partial Dependency Profile</a></li>
<li class="chapter" data-level="19.3.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#conditional-dependency-profile"><i class="fa fa-check"></i><b>19.3.2</b> Conditional Dependency Profile</a></li>
<li class="chapter" data-level="19.3.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#accumulated-local-profile"><i class="fa fa-check"></i><b>19.3.3</b> Accumulated Local Profile</a></li>
<li class="chapter" data-level="19.3.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#summaryFeatureEffects"><i class="fa fa-check"></i><b>19.3.4</b> Comparison of Explainers for Feature Effects</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#CDPExample"><i class="fa fa-check"></i><b>19.4</b> Example: Apartments data</a></li>
<li class="chapter" data-level="19.5" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPProsCons"><i class="fa fa-check"></i><b>19.5</b> Pros and cons</a></li>
<li class="chapter" data-level="19.6" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPR"><i class="fa fa-check"></i><b>19.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html"><i class="fa fa-check"></i><b>20</b> Residual Diagnostic</a><ul>
<li class="chapter" data-level="20.1" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#introduction-1"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#intuition"><i class="fa fa-check"></i><b>20.2</b> Intuition</a></li>
<li class="chapter" data-level="20.3" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#code-snippets-for-r"><i class="fa fa-check"></i><b>20.3</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="use-cases.html"><a href="use-cases.html"><i class="fa fa-check"></i>Use Cases</a></li>
<li class="chapter" data-level="21" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html"><i class="fa fa-check"></i><b>21</b> FIFA 19</a><ul>
<li class="chapter" data-level="21.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#introduction-2"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#data-preparation-1"><i class="fa fa-check"></i><b>21.2</b> Data preparation</a></li>
<li class="chapter" data-level="21.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#data-understanding"><i class="fa fa-check"></i><b>21.3</b> Data understanding</a></li>
<li class="chapter" data-level="21.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#model-assembly-1"><i class="fa fa-check"></i><b>21.4</b> Model assembly</a></li>
<li class="chapter" data-level="21.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#model-audit"><i class="fa fa-check"></i><b>21.5</b> Model audit</a></li>
<li class="chapter" data-level="21.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#model-understanding-1"><i class="fa fa-check"></i><b>21.6</b> Model understanding</a></li>
<li class="chapter" data-level="21.7" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#instance-understanding"><i class="fa fa-check"></i><b>21.7</b> Instance understanding</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendixes.html"><a href="appendixes.html"><i class="fa fa-check"></i>Appendixes</a></li>
<li class="chapter" data-level="22" data-path="conceptDrift.html"><a href="conceptDrift.html"><i class="fa fa-check"></i><b>22</b> Concept Drift</a><ul>
<li class="chapter" data-level="22.1" data-path="conceptDrift.html"><a href="conceptDrift.html#DriftIntro"><i class="fa fa-check"></i><b>22.1</b> Introduction</a></li>
<li class="chapter" data-level="22.2" data-path="conceptDrift.html"><a href="conceptDrift.html#DriftIntuition"><i class="fa fa-check"></i><b>22.2</b> Intuition</a></li>
<li class="chapter" data-level="22.3" data-path="conceptDrift.html"><a href="conceptDrift.html#DriftMethod"><i class="fa fa-check"></i><b>22.3</b> Method</a></li>
<li class="chapter" data-level="22.4" data-path="conceptDrift.html"><a href="conceptDrift.html#covariate-drift"><i class="fa fa-check"></i><b>22.4</b> Covariate Drift</a></li>
<li class="chapter" data-level="22.5" data-path="conceptDrift.html"><a href="conceptDrift.html#DriftExample"><i class="fa fa-check"></i><b>22.5</b> Example: Titanic data</a></li>
<li class="chapter" data-level="22.6" data-path="conceptDrift.html"><a href="conceptDrift.html#DriftProsCons"><i class="fa fa-check"></i><b>22.6</b> Pros and cons</a></li>
<li class="chapter" data-level="22.7" data-path="conceptDrift.html"><a href="conceptDrift.html#DriftR"><i class="fa fa-check"></i><b>22.7</b> Code snippets for R</a></li>
<li class="chapter" data-level="22.8" data-path="conceptDrift.html"><a href="conceptDrift.html#residual-drift"><i class="fa fa-check"></i><b>22.8</b> Residual Drift</a></li>
<li class="chapter" data-level="22.9" data-path="conceptDrift.html"><a href="conceptDrift.html#code-snippets"><i class="fa fa-check"></i><b>22.9</b> Code snippets</a></li>
<li class="chapter" data-level="22.10" data-path="conceptDrift.html"><a href="conceptDrift.html#model-drift"><i class="fa fa-check"></i><b>22.10</b> Model Drift</a></li>
<li class="chapter" data-level="22.11" data-path="conceptDrift.html"><a href="conceptDrift.html#code-snippets-1"><i class="fa fa-check"></i><b>22.11</b> Code snippets</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="data-set-hr.html"><a href="data-set-hr.html"><i class="fa fa-check"></i><b>23</b> Data Set HR</a><ul>
<li class="chapter" data-level="23.1" data-path="data-set-hr.html"><a href="data-set-hr.html#HFDataset"><i class="fa fa-check"></i><b>23.1</b> Hire or fire</a><ul>
<li class="chapter" data-level="23.1.1" data-path="data-set-hr.html"><a href="data-set-hr.html#exploration-HR"><i class="fa fa-check"></i><b>23.1.1</b> Data exploration</a></li>
<li class="chapter" data-level="23.1.2" data-path="data-set-hr.html"><a href="data-set-hr.html#model-HR-mr"><i class="fa fa-check"></i><b>23.1.2</b> Multinomial logistic regression</a></li>
<li class="chapter" data-level="23.1.3" data-path="data-set-hr.html"><a href="data-set-hr.html#model-HR-rf"><i class="fa fa-check"></i><b>23.1.3</b> Random forest</a></li>
<li class="chapter" data-level="23.1.4" data-path="data-set-hr.html"><a href="data-set-hr.html#predictionsHR"><i class="fa fa-check"></i><b>23.1.4</b> Model predictions</a></li>
<li class="chapter" data-level="23.1.5" data-path="data-set-hr.html"><a href="data-set-hr.html#ListOfModelsHR"><i class="fa fa-check"></i><b>23.1.5</b> List of objects for the <code>HR</code> example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="ccUseCase.html"><a href="ccUseCase.html"><i class="fa fa-check"></i><b>24</b> Use Case: Call Center</a><ul>
<li class="chapter" data-level="24.1" data-path="ccUseCase.html"><a href="ccUseCase.html#introduction-3"><i class="fa fa-check"></i><b>24.1</b> Introduction</a></li>
<li class="chapter" data-level="24.2" data-path="ccUseCase.html"><a href="ccUseCase.html#iteration-1-crisp-modeling"><i class="fa fa-check"></i><b>24.2</b> Iteration 1: Crisp modeling</a><ul>
<li class="chapter" data-level="24.2.1" data-path="ccUseCase.html"><a href="ccUseCase.html#data-preparation-2"><i class="fa fa-check"></i><b>24.2.1</b> Data preparation</a></li>
<li class="chapter" data-level="24.2.2" data-path="ccUseCase.html"><a href="ccUseCase.html#data-exploration-1"><i class="fa fa-check"></i><b>24.2.2</b> Data exploration</a></li>
<li class="chapter" data-level="24.2.3" data-path="ccUseCase.html"><a href="ccUseCase.html#model-assembly-2"><i class="fa fa-check"></i><b>24.2.3</b> Model assembly</a></li>
<li class="chapter" data-level="24.2.4" data-path="ccUseCase.html"><a href="ccUseCase.html#model-understanding-2"><i class="fa fa-check"></i><b>24.2.4</b> Model understanding</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="ccUseCase.html"><a href="ccUseCase.html#iteration-2-fine-tuning"><i class="fa fa-check"></i><b>24.3</b> Iteration 2: Fine tuning</a><ul>
<li class="chapter" data-level="24.3.1" data-path="ccUseCase.html"><a href="ccUseCase.html#analysis-of-residuals"><i class="fa fa-check"></i><b>24.3.1</b> Analysis of residuals</a></li>
<li class="chapter" data-level="24.3.2" data-path="ccUseCase.html"><a href="ccUseCase.html#sensitivity-analysis"><i class="fa fa-check"></i><b>24.3.2</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="24.3.3" data-path="ccUseCase.html"><a href="ccUseCase.html#deeper-analysis-of-individual-observations"><i class="fa fa-check"></i><b>24.3.3</b> Deeper analysis of individual observations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/pbiecek/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Models: Explore, Explain, and Debug</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelPerformance" class="section level1">
<h1><span class="header-section-number">Chapter 16</span> Model Performance Measures</h1>
<div id="modelPerformanceIntro" class="section level2">
<h2><span class="header-section-number">16.1</span> Introduction</h2>
<p>In this chapter, we present measures that are useful for the evaluation of a predictive model performance. They may be applied for several purposes:</p>
<ul>
<li>model evaluation: we may want to know how good is the model, i.e., how reliable are the model predictions (how frequent and how large errors we may expect);</li>
<li>model comparison: we may want to compare two or more models in order to choose between them;</li>
<li>out-of-sample and out-of-time comparisons: we may want to check model’s performance when applied to new data to evaluate if the performance has not worsened.</li>
</ul>
<p>Depending of the nature of the dependent variable (continuous, binary, categorical, count, etc.), different model performance measures may be used. Moreover, the list of useful measures is growing as new applications emerge. In this chapter, we focus on a selected set of measures that are used in model-level exploration techniques that are introduced in subsequent chapters.</p>
</div>
<div id="modelPerformanceIntuition" class="section level2">
<h2><span class="header-section-number">16.2</span> Intuition</h2>
<p>Most model performance measures are based on comparison of the model predictions with the (known) values of the dependent variable. For an ideal model, the predictions and the dependent-variable values should be equal. In practice, it is never the case, and we want to quantify the disagreement.</p>
<p>In applications, we can weigh differently the situation when the prediction is, for instance, larger than the true value, as compared to the case when it is smaller. Depending on the decision how to weigh different types of disagreement we may need different performance measures.</p>
<p>When assessing model’s performance, it is important to take into account the risk of overestimation of the performance when considering the data that were used for developing of the model. To mitigate the risk, various assessment stategies, such as cross-validation, have been proposed (see <span class="citation">(Kuhn and Johnson <a href="#ref-AppliedPredictiveModeling2013">2013</a><a href="#ref-AppliedPredictiveModeling2013">b</a>)</span>). In what follows, we consider the simple train-test-split strategy, i.e., we assume that the available data are split into a training set and a testing set. Model is created on the training set, and the testing set is used to assess the model’s performance.</p>
<!-- In the best possible scenario we can specify a single model performance measure before the model is created and then we optimize model for this measure. But in practice the more common scenario is to have few performance measures that are often selected after the model is created.
-->
</div>
<div id="modelPerformanceMethod" class="section level2">
<h2><span class="header-section-number">16.3</span> Method</h2>
<p>Assume that we have got a testing dataset with <span class="math inline">\(n\)</span> observations on <span class="math inline">\(p\)</span> explanatory variables and on a dependent variable <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(x_i\)</span> denote the (column) vector of values of the explanatory variables for the <span class="math inline">\(i\)</span>-th observation, and <span class="math inline">\(y_i\)</span> the corresponding value of the dependent variable. Denote by <span class="math inline">\(\widehat{y}_i=f(x_i)\)</span> model’s <span class="math inline">\(f()\)</span> prediction corresponding to <span class="math inline">\(y_i\)</span>. Let <span class="math inline">\(X=(x&#39;_1,\ldots,x&#39;_n)\)</span> denote the matrix of covariates for all <span class="math inline">\(n\)</span> observations, and <span class="math inline">\(y=(y_1,\ldots,y_n)&#39;\)</span> denote the (column) vector of the values of the dependent variable.</p>
<div id="modelPerformanceMethodCont" class="section level3">
<h3><span class="header-section-number">16.3.1</span> Continuous dependent variable</h3>
<p>The most popular model performance measure for models for a continuous dependent variable is the mean square error, defined as</p>
<p><span class="math display" id="eq:MSE">\[\begin{equation}
MSE(f,X,y) = \frac{1}{n} \sum_{i}^{n} \{f(x_i) - y_i\}^2 = r_i^2,
\tag{16.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(r_i=f(x_i) - y_i\)</span> is the residual for the <span class="math inline">\(i\)</span>-th observation. Thus, MSE can be seen as a sum of squared residuals. MSE is a convex differentiable function, which is important from an optimization point of view. As the measure weighs all differences equally, large residuals have got high impact on MSE. Thus, the measure is sensitive to outliers. For a ‘’perfect’’ predictive model, which predicts all <span class="math inline">\(y_i\)</span> exactly, <span class="math inline">\(MSE = 0\)</span>.</p>
<p>Note that MSE is constructed on a different scale than the dependent variable. Thus, a more interpretable variant of this measure is the root mean square error (RMSE), defined as</p>
<p><span class="math display" id="eq:RMSE">\[\begin{equation}
RMSE(f, X, y) = \sqrt{MSE(f, X, y)}.
\tag{16.2}
\end{equation}\]</span></p>
<p>A popular variant of RMSE is its normalized version, <span class="math inline">\(R^2\)</span>, defined as</p>
<p><span class="math display" id="eq:R2">\[\begin{equation}
R^2(f, X, y) = 1 - \frac{MSE(f, X, y)}{MSE(f_0, X,y)}.
\tag{16.3}
\end{equation}\]</span></p>
<p>In <a href="modelPerformance.html#eq:R2">(16.3)</a>, <span class="math inline">\(f_0()\)</span> is a ‘’baseline’’ model. For instance, in the case of the classical linear regression, <span class="math inline">\(f_0()\)</span> is the model that includes only the intercept, which implies the use of the average value of <span class="math inline">\(Y\)</span> as a prediction for all observations. <span class="math inline">\(R^2\)</span> is normalized in the sense that the ‘’perfect’’ predictive model leads to <span class="math inline">\(R^2 = 1\)</span>, while <span class="math inline">\(R^2 = 0\)</span> means that we are not doing better than the baseline model. In the context of the classical linear regresssion, <span class="math inline">\(R^2\)</span> is the familiar coefficient of determination and can be interpreted as the fraction of the total variance of <span class="math inline">\(Y\)</span> explained by model <span class="math inline">\(f()\)</span>.</p>
<p>Given sensitivity of MSE to outliers, sometimes the mean absolute error (MAE) is considered as a model performance measure:</p>
<p><span class="math display" id="eq:MAE">\[\begin{equation}
MAE(f, X ,y) = \frac{1}{n} \sum_{i}^{n} |f(x_i) - y_i| = \frac{1}{n} \sum_{i}^{n} |r_i|.
\tag{16.4}
\end{equation}\]</span></p>
<p>MAE is more robust to outliers than MSE. A disadvantage of MAE are its less favorable mathematical properties.</p>
</div>
<div id="modelPerformanceMethodBin" class="section level3">
<h3><span class="header-section-number">16.3.2</span> Binary dependent variable</h3>
<p>To introduce model performance measures, we label the two possible values of the dependent variable as <code>success</code> and <code>failure</code>. We also assume that model prediction <span class="math inline">\(f(x_i)\)</span> takes the form of the predicted probability of success.</p>
<p>If, additionally, we assign the value of 1 to <code>success</code> and 0 to <code>failure</code>, it is possible to use MSE, RMSE, and MAE, as defined in <a href="modelPerformance.html#eq:MSE">(16.1)</a>, <a href="modelPerformance.html#eq:RMSE">(16.2)</a>, <a href="modelPerformance.html#eq:MAE">(16.4)</a>, respectively, as a model performance measure. In practice, however, those summary measures are not often used. One of the main reasons is that they penalize too mildly for wrong predictions. In fact, the maximum penalty for an individual prediction is equal to 1 (if, for instance, the model yields zero probability for an actual success).</p>
<p>To address this issue, the log-likelihood function based on the Bernoulli distibution can be used:</p>
<p><span class="math display" id="eq:bernoulli">\[\begin{equation}
l(f, X ,y) =  -\sum_{i=1}^{n} [y_i \ln\{f(x_i)\}+ (1-y_i) \ln\{1-f(x_i)\}].
\tag{16.5}
\end{equation}\]</span></p>
<p>Note that, in the machine-learning world, often <span class="math inline">\(l(f, X ,y)/n\)</span> is considered (sometimes also with <span class="math inline">\(\ln\)</span> replaced by <span class="math inline">\(\log_2\)</span>) and termed ‘’logloss’’ or ‘’cross-entropy’‘. The log-likelihood heavily’‘penalizes’’ the cases when the model-predicted probability of success <span class="math inline">\(f(x_i)\)</span> is high for an actual failure (<span class="math inline">\(y_i=0\)</span>) and low for an actual success (<span class="math inline">\(y_i=1\)</span>).</p>
<p>In many situations, however, a consequence of a prediction error depends on the form of the error. For this reason, performance measures based on the (estimated values of) probability of correct/wrong prediction are more often used. To introduce some of those measures, we assume that, for each observation from the testing dataset, the predicted probability of success <span class="math inline">\(f(x_i)\)</span> is compared to a fixed cut-off threshold, <span class="math inline">\(C\)</span> say. If the probability is larger than <span class="math inline">\(C\)</span>, then we assume that the model predicts success; otherwise, we assume that it predicts failure. As a result of such a procedure, the comparison of the observed and predicted values of the dependent variable for the <span class="math inline">\(n\)</span> observations in the testing dataset can be summarized in the following table:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>True value: <code>success</code></th>
<th>True value: <code>failure</code></th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predicted: <code>success</code></td>
<td>True Positive: TP</td>
<td>False Positive (type I error): FP</td>
<td>P</td>
</tr>
<tr class="even">
<td>Predicted: <code>failure</code></td>
<td>False Negative (type II error): FN</td>
<td>True Negative: TN</td>
<td>N</td>
</tr>
<tr class="odd">
<td>Total</td>
<td>S</td>
<td>F</td>
<td><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
<p>In machine-learning world, the table is often referred to as the ‘’confusion table’’ or ‘’confusion matrix’‘. In statsicts, it is often called the’‘decision table’’. The counts <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span> on the diagonal of the table correspond to the cases when the predicted and observed value of the dependent variable <span class="math inline">\(Y\)</span> coincide. <span class="math inline">\(FP\)</span> is the number of cases in which failure is predicted as success. These are false-positive, or type I error, cases. On the other hand, <span class="math inline">\(FN\)</span> is the count of false-negative, or type II error, cases, in which success is predicted as failure. Marginally, there are <span class="math inline">\(P\)</span> predicted successes and <span class="math inline">\(N\)</span> predicted failures, with <span class="math inline">\(P+N=n\)</span>. In the testing dataset, there are <span class="math inline">\(S\)</span> observed sucesses and <span class="math inline">\(F\)</span> observed failures, with <span class="math inline">\(S+N=n\)</span>.</p>
<p>The simplest measure of model performance is <strong>accuracy</strong>, defined as</p>
<p><span class="math display">\[
ACC = \frac{TP+TN}{n}.
\]</span>
It is the fraction of correct predictions in the entire testing dataset. Accuracy is of interest if true positives and true negatives are more important than their false counterparts. However, accuracy may not be very informative when one of the binary categories (successes or failure) is much more prevalent. For example, if the testing data contain 90% of successes, a model that would always predict a success would reach accuracy of 0.9, although one could argue that this is not a very useful model.</p>
<p>There may be situations when false positives and/or false negatives may be of more concern. In that case, one might want to keep their number low. Hence, other measures, focused on the false results, might be of interest.</p>
<p>In the machine-learning world, two other measures are often considered: <strong>precision</strong> and <strong>recall</strong>. Precision is defined as</p>
<p><span class="math display">\[
Precision = \frac{TP}{TP+FP} = \frac{TP}{P}.
\]</span>
Precision is also referred to as the positive predictive value. It is the fraction of correct predictions among the predicted successes. Precision is high if the number of false positives is low. Thus, it is a useful measure when the penalty for committing the type I error (false positive) is high. For instance, consider the use of a genetic test in cancer diagnostics, with a positive result of the test taken as an indication of an increased risk of developing a cancer. A false positive result of a genetic test might mean that a person would have to unnecessarily cope with emotions and, possibly, medical procedures, related to the fact of being evaluated as having a high risk of developing a cancer. We might want to avoid this situation more than the false negative case. The latter would mean that the genetic test gives a negative result for a person that, actually, might be at an increased risk of developing a cancer. However, an increased risk does not mean that the person will develop cancer. And even so, we could hope that we could detect it in due time.</p>
<p>Recall is defined as</p>
<p><span class="math display">\[
Recall = \frac{TP}{TP+FN} = \frac{TP}{S}.
\]</span>
Recall is also referred to as sensitivity or true positive rate. It is the fraction of correct predictions among the true successes. Recall is high if the number of false negatives is low. Thus, it is a useful measure when the penalty for committing the type II error (false negative) is high. For instance, consider the use of an algorithm that predicts whether a bank transaction is fraudulent. A false negative result means that the algorithm accepts a fraudulent transaction as a legitimate one. Such a decision may have immediate and unpleasant consequences for the bank, because it may imply a non-recoverable loss of money. On the other hand, a false positive result means that a legitimate transaction is considered as fraudulent one and is blocked. However, upon further checking, the legitimate nature of the transaction can be confirmed with, perhaps, annoyed client as the only consequence for the bank.</p>
<p>The harmonic mean of these two measures defines the <strong>F1 score</strong>:</p>
<p><span class="math display">\[
F1\  score = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}} = 2\cdot\frac{Precision \cdot Recall}{Precision + Recall}.
\]</span>
F1 score tends to give a low value if either precision or recall is low, and a high value if both precision and recall are high. For instance, if precision is 0, F1 score will also be 0 irrespectively of the value of recall. Thus, it is a useful measure if we have got to seek a balance between precision and recall.</p>
<p>In statistics, and especially in applications in medicine, the popular measures are <strong>sensitivity</strong> and <strong>specificity</strong>. Sensitivity is simply another name for recall. Specificity is defined as</p>
<p><span class="math display">\[
Specificity = \frac{TN}{TN + FP} = \frac{TN}{F}.
\]</span>
Specificity is also referred to as true negative rate. It is the fraction of correct predictions among the true failures. Specificity is high if the number of false positives is low. Thus, as precision, it is a useful measure when the penalty for committing the type I error (false positive) is high.</p>
<p>The reason why sensitivity and specificity may be more often used outside the machine-learning world is related to the fact that their values do not depend on the proportion <span class="math inline">\(S/n\)</span> (sometimes termed ‘’prevalence’’) of true successess. This means that, once estimated in a sample obtained from one population, they may be applied to other populations, in which the prevalence may be different. This is not true for precision, because one can write</p>
<p><span class="math display">\[
Precision = \frac{Sensitivity \cdot \frac{S}{n}}{Sensitivity \cdot \frac{S}{n}+Specificity \cdot \left(1-\frac{S}{n}\right)}.
\]</span></p>
<p>All the measures depend on the choice of cut-off <span class="math inline">\(C\)</span>. To assess the form and the strength of dependence, a common approach is to construct the Receiver Operating Characteristic (ROC) curve. The curve plots the sensitivity in function of 1-specificity for all possible, ordered values of <span class="math inline">\(C\)</span>. Figure <a href="modelPerformance.html#fig:exampleROC">16.1</a> presents the ROC curve for the random-forest model for the Titanic dataset (see Section <a href="dataSetsIntro.html#model-titanic-rf">5.1.3</a>). Note that the curve indicates an inverse relationship between sensitivity and specifcity: by increasing one measure, the other is decreased.</p>
<div class="figure" style="text-align: center"><span id="fig:exampleROC"></span>
<img src="figure/ROCcurve.png" alt="(fig:exampleROC) ROC curve for the random-forest model for the Titanic dataset. The Gini ceofficient can be calculated as 2 x area between the ROC curve and the diagonal (this area is highlighted)." width="80%" />
<p class="caption">
Figure 16.1: (fig:exampleROC) ROC curve for the random-forest model for the Titanic dataset. The Gini ceofficient can be calculated as 2 x area between the ROC curve and the diagonal (this area is highlighted).
</p>
</div>
<p>The ROC curve is very informative. For a model that predicts successes and failures at random, the corresponding ROC curve will be equal to the diagonal line. On the other hand, for a model that yields perfect predictions, the ROC curve reduces to a two intervals that connect points (0,0), (0,1), and (1,1).</p>
<p>Often, there is a need to summarize the ROC curve and, hence, model’s performance. A popular measure that is used toward this aim is the area under the curve (AUC). For a model that predicts successes and failures at random, AUC is the area under the diagonal line, i.e., it is equal to 0.5. For a model that yields perfect predictions, AUC is equal to 1.</p>
<p>Another ROC-curve-based measure that is often used is the Gini coefficient <span class="math inline">\(G\)</span>. It is closely related to AUC; in fact, it can be calculated a <span class="math inline">\(G = 2 \times AUC - 1\)</span>. For a model that predicts successes and failures at random, <span class="math inline">\(G=0\)</span>; for a perfect-prediction model, <span class="math inline">\(G = 1\)</span>.</p>
<p>The value of Gini’s coefficient or, equivalently, of <span class="math inline">\(AUC-0.5\)</span> allow a comparison of the model-based predictions with random guessing. A measure that explicitly compares a prediction model with a baseline (or null) model is the <strong>lift</strong>. Commonly, as the baseline model, random guessing is considered. In that case,</p>
<p><span class="math display">\[
Lift  = \frac{\frac{TP}{P}}{\frac{S}{n}} = \frac{Precision}{\frac{S}{n}}.
\]</span>
Note that <span class="math inline">\(S/n\)</span> can be seen as the estimated probability of a correct prediction of a success for random guessing. n the other hand, <span class="math inline">\(TP/P\)</span> is the estimated probability of a correct prediction a succcess given that the model predicts a success. Hence, informally speaking, if the lift indicates how many more (or less) times the model does better in predicting success than random quessing. As other measures, the lift depends on the choice of cut-off <span class="math inline">\(C\)</span>. The plot of the lift as a function of <span class="math inline">\(C\)</span> is called the lift chart.</p>
<p>There are many more measures aimed at measuring performance of a predictive model for a inary dependent variable. An overview can be found in, e.g., (Berrar D. Performance Measures for Binary Classification. Encyclopedia of Bioinformatics and Computational Biology Volume 1, 2019, Pages 546-560).</p>
</div>
<div id="modelPerformanceMethodCateg" class="section level3">
<h3><span class="header-section-number">16.3.3</span> Categorical dependent variable</h3>
<p>To introduce model performance measures, we assume that <span class="math inline">\(y_i\)</span> is now a vector of <span class="math inline">\(K\)</span> elements. Each element <span class="math inline">\(y_{ik}\)</span> (<span class="math inline">\(k=1,\ldots,K\)</span>) is a binary variable indicating whether the <span class="math inline">\(k\)</span>-th category was observed for the <span class="math inline">\(i\)</span>-th observation. We assume that for each observation only one category can be observed. Thus, all elements of <span class="math inline">\(y_i\)</span> are equal to 0 except of one that is equal to 1. Furthermore, We assume that model prediction <span class="math inline">\(f(x_i)\)</span> takes the form of a vector of the predicted probabilities for each of the <span class="math inline">\(K\)</span> categories. The predicted category is the one with the highest predicted probability.</p>
<p>The log-likelihood function <a href="modelPerformance.html#eq:bernoulli">(16.5)</a> can be adapted to the categorical dependent variable case as follows:</p>
<p><span class="math display" id="eq:multinom">\[\begin{equation}
l(f, X ,y) =  -\sum_{i=1}^{n}\sum_{k=1}^{K} y_{ik} \ln\{f(x_i)_k\}.
\tag{16.6}
\end{equation}\]</span></p>
<p>It is essentially the log-likelihood function based on a multinomial distibution.</p>
<p>It is also possible to extend the performance measures like accuracy, precision, etc., introduced in Section <a href="modelPerformance.html#modelPerformanceMethodBin">16.3.2</a>. Toward this end, first, a confusion table is created for each category <span class="math inline">\(k\)</span>, treating the category as ‘’success’’ and all other categories as ‘’failure’’. Let us denote the counts in the table by <span class="math inline">\(TP_k\)</span>, <span class="math inline">\(FP_k\)</span>, <span class="math inline">\(TN_k\)</span>, and <span class="math inline">\(FN_k\)</span>. Based on the counts, we can compute the average accuracy across all classes as follows:</p>
<p><span class="math display" id="eq:accmacro">\[\begin{equation}
\overline{ACC} = \frac{1}{K}\sum_{k=1}^K\frac{TP_k+TN_k}{n}.
\tag{16.7}
\end{equation}\]</span></p>
<p>Similarly, one could compute the average precision, average sensitivity, etc. In machine-learning world, this approach is often termed ‘’macro-averaging’’. The averages computed in that way treat all classes equally.</p>
<p>An alternative approach is to sum the appropriate counts from the confusion tables for all classes, and then form a measure based on so-computed cumulative counts. For instance, for precision, this would lead to</p>
<p><span class="math display" id="eq:precmicro">\[\begin{equation}
\overline{Precision}_{\mu} = \frac{\sum_{k=1}^K TP_k}{\sum_{k=1}^K (TP_k+FP_k)}.
\tag{16.8}
\end{equation}\]</span></p>
<p>In machine-learning world, this approach is often termed ‘’micro-averaging’’ (hence subscript <span class="math inline">\(\mu\)</span> for ‘’micro’’ in <span class="math inline">\(Precision_{\mu}\)</span> in <a href="modelPerformance.html#eq:precmicro">(16.8)</a>). Note that, for accuracy, this computation still leads to <a href="modelPerformance.html#eq:accmacro">(16.7)</a>. The measures computed in that way favor classes with larger numbers of observations.</p>
</div>
<div id="modelPerformanceMethodCount" class="section level3">
<h3><span class="header-section-number">16.3.4</span> Count dependent variable</h3>
<p>In case of counts, one could consider using any of the measures for a continuous dependent variable mentioned in Section <a href="modelPerformance.html#modelPerformanceMethodCont">16.3.1</a>. However, a particular feature of a count dependent variable is that, often, its variance depends on the mean value. Consequently, weighing all contributions to MSE equally, as in <a href="modelPerformance.html#eq:MSE">(16.1)</a>, is not appropriate, because the same residual value <span class="math inline">\(r_i\)</span> indicates a larger discrepancy for a smaller count <span class="math inline">\(y_i\)</span> than for a larger one. Therefore, a popular measure is of performance of a predictive model for counts is Pearson’s statistic:</p>
<p><span class="math display" id="eq:Pearson">\[\begin{equation}
\chi^2(f,X,y) = \sum_{i}^{n} \left\{\frac{f(x_i) - y_i}{\sqrt{f(x_i)}}\right\}^2 = \sum_{i}^{n} \left\{\frac{r_i}{\sqrt{f(x_i)}}\right\}^2.
\tag{16.9}
\end{equation}\]</span></p>
<p>From <a href="modelPerformance.html#eq:Pearson">(16.9)</a> it is clear that, if the same residual value is obtained for two different observed counts, it is assigned a larger weight for the count for which the predicted value is smaller.</p>
</div>
</div>
<div id="example" class="section level2">
<h2><span class="header-section-number">16.4</span> Example</h2>
<div id="modelPerformanceApartments" class="section level3">
<h3><span class="header-section-number">16.4.1</span> Apartments data</h3>
<p>Let us consider the linear regression model <code>apartments_lm_v5</code> (see Section <a href="dataSetsIntro.html#model-Apartments-lr">5.2.2</a>) and the random-forest model <code>apartments_rf_v5</code> (see Section <a href="dataSetsIntro.html#model-Apartments-rf">5.2.3</a>) for the data on the apartment prices (see Section <a href="dataSetsIntro.html#ApartmentDataset">5.2</a>). Recall that, for these data, the dependent variable, the price, is continuous. Hence, we can use the performance measures presented in Section <a href="modelPerformance.html#modelPerformanceMethodCont">16.3.1</a>. In particular, we consider MSE and MAE. The values of the two measures for the two models are presented below.</p>
<pre><code>## Model label:  Linear Regression v5 
##          score name
## mse 78023.1235  mse
## mae   260.0254  mae

## Model label:  Random Forest v5 
##          score name
## mse 36669.1954  mse
## mae   144.0888  mae</code></pre>
<p>Both MSE and MAE indicate that, overall, the random-forest model performs better than the linear regression model.</p>
</div>
<div id="modelPerformanceTitanic" class="section level3">
<h3><span class="header-section-number">16.4.2</span> Titanic data</h3>
<p>Let us consider the random-forest model <code>titanic_rf_v6</code> (see Section <a href="dataSetsIntro.html#model-titanic-rf">5.1.3</a> and the logistic regression model <code>titanic_lmr_v6</code> (see Section <a href="dataSetsIntro.html#model-titanic-lmr">5.1.2</a>) for the Titanic data (see Section <a href="dataSetsIntro.html#TitanicDataset">5.1</a>). Recall that, for these data, the dependent variable is binary, with success defined as survival of the passenger.</p>
<p>First, we will take a look at the accuracy, F1 score, and AUC for the models.</p>
<pre><code>## Model label:  Logistic Regression v6 
##         score name
## auc 0.8196991  auc
## f1  0.6589018   f1
## acc 0.8046689  acc

## Model label:  Random Forest v6 
##         score name
## auc 0.8566304  auc
## f1  0.7289880   f1
## acc 0.8494521  acc</code></pre>
<p>Overall, the random-forest model is performing better, as indicated by the larger values of all the measures.</p>
<p>Figure <a href="modelPerformance.html#fig:titanicROC">16.2</a> presents ROC curves for both models. The curve for the random-forest model lies above the one for the logistic regression model for the majority of the cut-offs <span class="math inline">\(C\)</span>, except for the very high values.</p>
<div class="figure"><span id="fig:titanicROC"></span>
<img src="PM_VEE_files/figure-html/titanicROC-1.png" alt="(fig:titanicROC) ROC curves for the  random-forest model and the logistic regression model for the Titanic dataset." width="480" />
<p class="caption">
Figure 16.2: (fig:titanicROC) ROC curves for the random-forest model and the logistic regression model for the Titanic dataset.
</p>
</div>
<p>Figure <a href="modelPerformance.html#fig:titanicLift">16.3</a> presents lift curves for both models. Also in this case the curve for the random-forest suggests a better performance than for the logistic regression model, except for the very high values of cut-ff <span class="math inline">\(C\)</span>. [TOMASZ: THIS CURVE IS NOT CONSISTENT WITH THE DEFINITION OF THE LIFT. EXPLAIN/CHANGE?]</p>
<div class="figure"><span id="fig:titanicLift"></span>
<img src="PM_VEE_files/figure-html/titanicLift-1.png" alt="(fig:titanicLift) Lift curves for the  random-forest model and the logistic regression model for the Titanic dataset." width="480" />
<p class="caption">
Figure 16.3: (fig:titanicLift) Lift curves for the random-forest model and the logistic regression model for the Titanic dataset.
</p>
</div>
</div>
</div>
<div id="modelPerformanceProsCons" class="section level2">
<h2><span class="header-section-number">16.5</span> Pros and cons</h2>
<p>All model performance measures presented in this chapter face some limitations. For that reason, many measures are available, as the limitations of a particular measure were addressed by developing an alternative. For instance, RMSE is frequently used and reported for linear regression models. However, as it is sensitive to outliers, MAE was proposed. In case of predictive models for a binary dependent variable, the measures like accuracy, F1 score, sensitivity, and specificity, are often considered depending on the consequences of corrrect/incorrect predictions in a particular application. However, the value of those measures depends on the cut-off value used for creating the predictions. For this reason, ROC curve and AUC have been developed and have become very popular. They are not easily extanded to the case of a categircal dependent variable, though.</p>
<p>Given the advantages and disadvantages of various measures, and the fact that each may reflect a different aspect of the predicitve performance of a model, it is customary to report and compare several of them when evaluating a model’s performance.</p>
</div>
<div id="modelPerformanceR" class="section level2">
<h2><span class="header-section-number">16.6</span> Code snippets for R</h2>
<p>In this section, we present the key features of the <code>auditor</code> R package <span class="citation">(<span class="citeproc-not-found" data-reference-id="auditor"><strong>???</strong></span>)</span> which is a part of the <a href="http://DrWhy.AI">DrWhy.AI</a> universe. The package covers all methods presented in this chapter. It is available on CRAN and GitHub. More details and examples can be found at <a href="https://modeloriented.github.io/auditor/" class="uri">https://modeloriented.github.io/auditor/</a>.</p>
<p>Note that there are also other R packages that offer similar functionality. These include packeges <code>mlr</code> <span class="citation">(Bischl et al. <a href="#ref-mlr">2016</a>)</span>, <code>caret</code> <span class="citation">(Jed Wing et al. <a href="#ref-caret">2016</a>)</span>, <code>tidymodels</code> <span class="citation">(Max and Wickham <a href="#ref-tidymodels">2018</a>)</span>, and <code>ROCR</code> <span class="citation">(Sing et al. <a href="#ref-ROCR">2005</a>)</span>.</p>
<p>For illustration purposes we use the random-forest model <code>titanic_rf_v6</code> (see Section <a href="dataSetsIntro.html#model-titanic-rf">5.1.3</a> and the logistic regression model <code>titanic_lmr_v6</code> (see Section <a href="dataSetsIntro.html#model-titanic-lmr">5.1.2</a>) for the Titanic data (see Section <a href="dataSetsIntro.html#TitanicDataset">5.1</a>).the random-forest model <code>titanic_rf_v6</code> (see Section <a href="#odel-HR-rf"><strong>??</strong></a>). Consequently, the functions from the <code>auditor</code> package are applied in the context of a binary classification problem. However, the same functions can be used for, e.g., linear regression problems.</p>
<p>To illustrate the use of the functions, we first load explainers for both models.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;auditor&quot;</span>)</a>
<a class="sourceLine" id="cb122-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</a>
<a class="sourceLine" id="cb122-3" data-line-number="3"></a>
<a class="sourceLine" id="cb122-4" data-line-number="4">explainer_titanic_rf &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="st"> </span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/51c50&quot;</span>)</a>
<a class="sourceLine" id="cb122-5" data-line-number="5">explainer_titanic_lr &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="st"> </span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/42d51&quot;</span>)</a></code></pre></div>
<p>Function <code>auditor::model_performance()</code> calculates selected model performance measures. The <code>score</code> argument is used to select the desired measures. The <code>data</code> argument serves for specification of the test dataset, for which the selected measures are to be computed. Note that, by default, the data are extracted from the explainer object. Finally, it is possible to use the <code>cutoff</code> argument to specify the cut-off value to obtained cut-off-dependent measures like F1 score or accuracy.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1"><span class="kw">model_performance</span>(explainer_titanic_rf, <span class="dt">score =</span> <span class="kw">c</span>(<span class="st">&quot;auc&quot;</span>, <span class="st">&quot;f1&quot;</span>, <span class="st">&quot;acc&quot;</span>))</a></code></pre></div>
<pre><code>## Model label:  Logistic Regression v6 
##         score name
## auc 0.8196991  auc
## f1  0.6589018   f1
## acc 0.8046689  acc</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1"><span class="kw">model_performance</span>(explainer_titanic_lr, <span class="dt">score =</span> <span class="kw">c</span>(<span class="st">&quot;auc&quot;</span>, <span class="st">&quot;f1&quot;</span>, <span class="st">&quot;acc&quot;</span>))</a></code></pre></div>
<pre><code>## Model label:  Random Forest v6 
##         score name
## auc 0.8566304  auc
## f1  0.7289880   f1
## acc 0.8494521  acc</code></pre>
<p>ROC or lift curves can be constructed by, first, using the <code>model_evaluation()</code> function. [TOMASZ: WHAT DOES IT DO?] Subseqeuntly, the resulting object is used in the <code>plot_roc()</code> or <code>plot_lift()</code> function calls. Both plot functions return <code>ggplot2</code> objects and can take one or more explainer objects as arguments. In the latter case, the profiles for each explainer are superimposed on one plot.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1">eva_rf &lt;-<span class="st"> </span><span class="kw">model_evaluation</span>(explainer_titanic_rf)</a>
<a class="sourceLine" id="cb127-2" data-line-number="2">eva_lr &lt;-<span class="st"> </span><span class="kw">model_evaluation</span>(explainer_titanic_lr)</a>
<a class="sourceLine" id="cb127-3" data-line-number="3"><span class="kw">plot_roc</span>(eva_rf, eva_lr)</a>
<a class="sourceLine" id="cb127-4" data-line-number="4"><span class="kw">plot_lift</span>(eva_rf, eva_lr)</a></code></pre></div>
<p>The resulting plots are shown in Figures <a href="modelPerformance.html#fig:titanicROC">16.2</a> and <a href="modelPerformance.html#fig:titanicLift">16.3</a>. Both plots can be supplemented with boxplots for residuals. Toward this end, the residuals have got to computed and added to the explainer object with the help of the <code>model_residual()</code> function. Subsequently, the <code>plot_residual_boxplot()</code> can be applied to the resulting object. [TOMASZ: CODE NOT WORKING FOR SOME REASON.]</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1"><span class="co"># mr_rf &lt;- model_residual(explainer_titanic_rf)</span></a>
<a class="sourceLine" id="cb128-2" data-line-number="2"><span class="co"># mr_lm &lt;- model_residual(explainer_titanic_lr)</span></a>
<a class="sourceLine" id="cb128-3" data-line-number="3"></a>
<a class="sourceLine" id="cb128-4" data-line-number="4"><span class="co"># plot_residual_boxplot(mr_rf, mr_lm)</span></a></code></pre></div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-mlr">
<p>Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” <em>Journal of Machine Learning Research</em> 17 (170): 1–5. <a href="http://jmlr.org/papers/v17/15-066.html">http://jmlr.org/papers/v17/15-066.html</a>.</p>
</div>
<div id="ref-caret">
<p>Jed Wing, Max Kuhn. Contributions from, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, et al. 2016. <em>Caret: Classification and Regression Training</em>. <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.</p>
</div>
<div id="ref-AppliedPredictiveModeling2013">
<p>Kuhn, Max, and Kjell Johnson. 2013b. <em>Applied Predictive Modeling</em>. Springer. <a href="http://appliedpredictivemodeling.com/">http://appliedpredictivemodeling.com/</a>.</p>
</div>
<div id="ref-tidymodels">
<p>Max, Kuhn, and Hadley Wickham. 2018. <em>Tidymodels: Easily Install and Load the ’Tidymodels’ Packages</em>. <a href="https://CRAN.R-project.org/package=tidymodels">https://CRAN.R-project.org/package=tidymodels</a>.</p>
</div>
<div id="ref-ROCR">
<p>Sing, T., O. Sander, N. Beerenwinkel, and T. Lengauer. 2005. “ROCR: Visualizing Classifier Performance in R.” <em>Bioinformatics</em> 21 (20): 7881. <a href="http://rocr.bioinf.mpi-sb.mpg.de">http://rocr.bioinf.mpi-sb.mpg.de</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modelLevelExploration.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="featureImportance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["PM_VEE.pdf", "PM_VEE.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
