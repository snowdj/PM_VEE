# Summary of Explainers for Feature Effects {#summaryFeatureEffects}

In previous chapters we introduced different was to calculate model level explainers for feature effects. 
A natural question is how these approaches are different and which one should we choose.

An example that illustrate differences between these approaches is presented in Figure \@ref{accumulatedLocalEffects}.
Here we have a model $f(x_1, x_2) = x_1*x_2 + x_2$ and what is important features are correlated $x_1 \sim U[-1,1]$ and $x_2 = x_1$.

We have 8 points for which we calculated instance level profiles.

| $x_1$  | $x_2$ |
|--------|-------|
|    -1  |   -1  |
| -0.71  | -0.71 |
| -0.43  | -0.43 |
| -0.14  | -0.14 |
|  0.14  |  0.14 |
|  0.43  |  0.43 |
|  0.71  |  0.71 |
|    1   |    1  |

Panel A) shows Ceteris Paribus for 8 data points, the feature $x_1$ is on the OX axis while $f$ is on the OY. 
Panel B) shows Partial Dependency Profiles calculated as an average from CP profiles.

$$
g_{PD}^{f,1}(z) = E[z*x^2 + x^2] = 0
$$
Panel C) shows Conditional Dependency Profiles calculated as an average from conditional CP profiles. In the figure the conditioning is calculated in four bins, but knowing the formula for $f$ we can calculated it directly as.

$$
g_{CD}^{f,1}(z) = E[X^1*X^2 + X^2 | X^1 = z] = z^2+z
$$

Panel D) shows Accumulated Local Effects calculated as accumulated changes in conditional CP profiles. In the figure the conditioning is calculated in four bins, but knowing the formula for $f$ we can calculated it directly as.

$$
g_{AL}^{f,1}(z) = \int_{z_0}^z E\left[\frac{\partial (X^1*X^2 + X^2)}{\partial x_1}|X^1 = v\right] dv  = \int_{z_0}^z E\left[X^2|X^1 = v\right] dv  = \frac{z^2 -1 }{2},
$$




```{r accumulatedLocalEffects, echo=FALSE, fig.cap="(fig:accumulatedLocalEffects) Differences between Partial Dependency, Marginal and Accumulated Local Effects profiles. Panel A) shows Ceteris Paribus Profiles for 8 points. Panel B) shows Partial Dependency profiles, i.e. an average out of these profiles. Panel C shows Marginal profiles, i.e. an average from profiles similar to the point that is being explained. Panel D shows Accumulated Local Effects, i.e. effect curve that takes into account only changes in the Ceteris Paribus Profiles.", out.width = '90%', fig.align='center'}
knitr::include_graphics("figure/CP_ALL.png")
```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(randomForest)
library(DALEX)
library(dplyr)

titanic_small <- titanic_imputed[,c("survived", "class", "gender", "age", "sibsp", "parch", "fare", "embarked")]
rf_model <- randomForest(survived ~ class + gender + age + sibsp + parch + fare + embarked, 
                         data = titanic_small)
predict_fuction <- function(m,x) predict(m, x, type="prob")[,2]
rf_explain <- explain(rf_model, data = titanic_small, 
                      y = titanic_small$survived == "yes", label = "RF",
                      predict_function = predict_fuction)


#
# TWORZYMY MODELE

## random forest
rf_model <- randomForest(survived ~ class + gender + age + sibsp + parch + fare + embarked,
                         data = titanic_small)
predict_rf_fuction <- function(m,x) predict(m, x, type="prob")[,2]
explainer_rf <- explain(rf_model, data = titanic_small,
                      y = titanic_small$survived == "yes", label = "RF",
                      predict_function = predict_rf_fuction)

## GLM
glm_model <- glm(survived ~ class + gender + age + sibsp + parch + fare + embarked,
                         data = titanic_small, family = "binomial")
explainer_glm <- explain(glm_model, data = titanic_small,
                      y = titanic_small$survived == "yes", label = "GLM")


## splines
library("rms")
rms_model <- lrm(survived == "yes" ~ class + gender + rcs(age) + sibsp +
                   parch + fare + embarked, titanic_small)
predict_rms_fuction <- function(m,x) predict(m, x, type="fitted")
explainer_rms <- explain(rms_model, data = titanic_small,
                         y = titanic_small$survived == "yes", label = "RMS",
                         predict_function = predict_rms_fuction)

## GBM
library("gbm")
#titanic_gbm <- gbm(Survived == "1" ~ Age + Pclass + Sex, data = titanic_small, n.trees = 1000)
titanic_gbm <- gbm(survived == "yes" ~ class + gender + age + sibsp +
                     parch + fare + embarked, data = titanic_small, n.trees = 15000)
predict_gbm_fuction <- function(m,x) predict(m, x,
                                             n.trees = 15000, type = "response")
explainer_gbm <- explain(titanic_gbm,
                         data = titanic_small, y = titanic_small$survived == "yes",
                         label = "GBM",
                         predict_function = predict_gbm_fuction)

```



## Merging Path Plots and Others  {#factorMerger}

[@demsar2018]

[@RJ2017016]
[@MAGIX]






[@R-factorMerger]


[@Strobl2007] 
[@Strobl2008] 
- variable importance

[@2018arXiv180101489F]

Beware Default Random Forest Importances

Terence Parr, Kerem Turgutlu, Christopher Csiszar, and Jeremy Howard
March 26, 2018.

http://explained.ai/rf-importance/index.html



```{r, warning=FALSE, message=FALSE}
library(factorMerger)
```

## Other topics

Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?
[@Edwards_Veale_2018]



[@R-randomForestExplainer]
[@R-ICEbox]
[@R-ALEPlot]

[@R-modelDown]

