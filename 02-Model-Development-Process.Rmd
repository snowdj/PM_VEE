# Model Development {#modelDevelopmentProcess}

## Introduction {#MDPIntro}

In this book we present methods that can be used for exploration and explanation of predictive models. But before we can explore a model, first we need to train one.

In this part of the book we overview the process of model development and introduce steps that lead to a model creation. It is not a comprehensive manual ,,how to train a model in 5 steps''. The goal of this chapter is to show what needs to be performed before we can do any diagnostic or exploration of a trained model.

Predictive models are created for different purposes. Sometimes it is a team of data scientists that spend months on a single model that will be used for model scoring in a big financial company. Every detail is important for models that operate on large scale and have long-term consequences. Another time it is an in-house model trained for prediction of a demand for pizza. The model is developed by a single person in few hours. If model will not perform well it will be updated, replaced or removed.

Whatever it is a large model or small one, similar steps are to be taken during model development.


## The Process 

Several approaches are proposed in order to describe the process of model development. Their main goal is to standardize the process. And the standardisation is important because it helps to plan resources needed to develop and maintain the model and also to not miss any important phase.

The most known methodology for data science projects is CRISP-DM [@crisp1999], [@crisp2019wiki] which is a tool agnostic procedure. The key component of CRISP-DM is the break down of the whole process into six phases, that are iterated: business understanding, data understanding, data preparation, modeling, evaluation and deployment. CRISP-DM is general, it was designed for any data science project. For predictive models some methodologies are introduced in [@r4ds2019] and [@misconceptions2019]. First is a very simple, focused on iteractions of three phases: data transformation, modeling and visualisation. 

Figure \@ref(fig:MDPwashmachine) presents this iterative process divided into five steps. This is the common thinking about model development. Repeat until convergence, or repeat until best model is identified.

```{r MDPwashmachine, echo=FALSE, fig.cap="Lifecycle of predictive model can be decomposed into six tasks. First we need data that is poured into the model development. The model development is highly iterative, learn something new about the data, assemble a new model based on current understanding, and validate the new model. Repeat these steps as long as needed to be satisfied with model performance. Once the model is created we can deliver the model to the production along with required tests and documentation.", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/MDP_washmachine.png")
```


In this book we use *Model Development Process* introduced in [@mdp2019]. It is motivated by Rational Unified Process for Software Development [@rup1998], [@usdp1999], [@spiral1988]. One can think about MDP as an extension of process introduced in Figure \@ref(fig:MDPwashmachine). What is important is to notice that consecutive iterations are not identical. Our knowledge increases during the process and consecutive iterations are performed with different goals in mind.  
This is why MDP is build an an untangled version of Figure \@ref(fig:MDPwashmachine). The MDP process is shown in Figure \@ref(fig:mdpGeneral). Each vertical stripe is a single run of the cycle. 
First iterations are usually focused on *formulation of the problem*. Sometimes the problem is well stated, however it's a rare situation valid maybe for kaggle competitions. In most real-life problems the problem formulation requires lots of discussions and experiments. Once the problem is defined we can start building first prototypes, *crisp versions of models*. These initial versions of models are to verify if the problem can be solved and how far we are form the solution. Usually we gather more information and go for the next phase  the *fine tuning*. We repeat these iterations until a final version of a model is developed. Then we move to the last phase *maintenance and one day decommissioning*.  

```{r mdpGeneral, echo=FALSE, fig.cap="Overview of the Model Development Process. Horizontal axis show how time passes from the problem formulation to the model decommissioning. Vertical axis shows tasks are performed in a given phase. Each veritical strip is a next iteration of cycle presented in Figure \\@(fig:MDPwashmachine)", out.width = '99%', fig.align='center'}
knitr::include_graphics("figure/mdp_general.png")
```

Having in mind the map of model development we can point places where one can use methods presented in this book.

As suggested in the title of this book, three primary applications are: exploration, explanation and debugging. *Exploration* refers to situations in which we better understand the data and the domain. Presented techniques can be used to speed up the variable engineering or variable selection. *Explanation* refers to situations in which we are interested in decision paths beyond particular predictions. *Debugging* refers to situations in which we want to understand week points of a model and correct them. These applications target phases Data understanding, Model assembly and Model audit.

In this book we present various examples based on three use cases, two introduced in Chapter \@ref(dataSetsIntro) and one in Chapter \@ref(UseCaseFIFA). Due to space limitation we do not show the full life cycle of these problems, but we are focused on phases Crisp modeling and Fine tuning.

Rest of this chapter is focused on a brief overview of the notation and commonly used methods for data exploration, model training and model validation. 


## Notation {#notation}

Methods described in this book were developed by different authors, who used different mathematical notations. 
We try to keep the mathematical notation consistent throughout the entre book. In some cases this may result in formulae with a fairly complex system of indices.

In this section, we provide a general overview of the notation we use. Whenever necessary, parts of the notation will be explained again in subsequent chapters.

We assume that the data consist $n$ observations/instances. Each observation is described by $p$ explanatory variables. Thus data is described as a set of points on a **$p$-dimensional input space** $\mathcal X \equiv \mathcal R^p$. By $x \in \mathcal X$ we will refer to a single point in this input space.
By $x_i$ we refer to the $i$-th observation in this dataset. Of course, $x_i \in \mathcal X$.

Some methods of model exploration are constructed around an observation of interest which will be denoted by $x_{*}$. The observation may not necessarily belong to the analyzed dataset; hence, the use of the asterisk in the index. Of course, $x_* \in \mathcal X$.

Points in $\mathcal X$ are $p$ dimensional vectors. We will refer to the $j$-th coordinate by using $j$ in superscript. Thus, $x^j_i$ deontes the $j$-th coordinate of the $i$-th observation from the analyzed dataset. If $\mathcal J$ denotes a subset of indices, then $x^{\mathcal J}$ denotes the elements of vector $x$ corresponding to the indices included in $\mathcal J$. 

We will use the notation $x^{-j}$ to refer to a vector that results from removing the $j$-th coordinate from vector $x$. By **$x^{j|=z}$**, we denote a vector with the values at all coordinates equal to the values in $x$, except of the $j$-th coordinate, which is set equal to $z$. So, if $w=x^{j|=z}$, then $w^j = z$ and $\forall_{k\neq j} w^k = x^k$. In other words $x^{j|=z} = (x^1, ..., x^{j-1}, z, x^{j+1}, ..., x^p)$.

In this book, a model is a function $f:\mathcal X \rightarrow \mathcal R$ that transforms a point from $\mathcal X$ into a real number. In most cases, the presented methods can be used directly for multi-variate dependent variables; however, we use examples with uni-variate responses to simplify the notation.
Typically, during the model development, we created many competing models. Formally we shall also index models to refer to a specific version of a trained model. But for the sake of simplicity we omit this index where it is not important.


We will use $r_i = y_i - f(x_i)$ we refer to the **model residual**, i.e., the difference between the observed value of the dependent variable $Y$ for the $i$-th observation from a particular dataset and the model prediction for the observaton.





## Data exploration 

Before we start the modeling we need to understand the data.
Visual, tabular and statistical tools for data exploration are used depending on the character of variables.

The most know introduction to data exploration is the famous book by John Tukey [@tukey1977]. It introduced new tools for data exploration, like for example boxplots for continuous variables. Availability of computational tools makes the process of data exploration easier and ore interactive. Find a good overview of techniques for data exploration in [@Nolan2015] or [@Wickham2017].


In this book we will relay on five visual methods for data exploration presented in Figure \@ref(fig:UMEPEDA). Two of them are used to present distribution of explanatory or target variables; three others are used to explore pairwise relations between variables.

```{r UMEPEDA, echo=FALSE, fig.cap="Basic methods for visual exploration. Histogram for distribution of continuous or categorical variables, empirical cumulative distribution for continuous variables. Mosaic plot for relation between two categorical variables, boxplots for relation between continuous and categorical variables or scatterplot for relation between two continuous variables.", out.width = '75%', fig.align='center'}
knitr::include_graphics("figure/UMEPEDA.png")
```

Distribution of categorical variable is summarized with a barplot, distribution of numerical variable is summarized with a histogram or empirical cumulative distribution function.

Primary goal for exploration of target variable is to decide if some variable transformation is needed (e.g. if the variable is skewed or with fat tails) or to verify if target variable is balances (because some methods are not working well with unbalanced data). Exploration of dependent variables is performed mainly to decide if any variable transformation is needed.

Relations between two variables, mostly between a single dependent variable and target variable, are visualized with mosaic plots (for two categorical variables), boxplots (for numerical and categorical variable) and scatter plots (for two numerical variables). Such exploration may provide some insights for variable selection/filtering (if the variable is not related with the target then variable may be removed from the model) or variable engineering (if from the exploration we gain information how a variable may be transformed).


## Model training

TODO: PBI


Once the data is prepared we can start model assembly. 

One can try different algorithms for model training, validation strategies, tuning of hyperparameters. This process is usually iterative and computationally heavy.

Find a good overview of techniques for model development in [@Venables2010] or [@AppliedPredictiveModeling2013].


## Model understanding

Usually the model development starts with some crisp early versions that are refined in consecutive iterations. In order to train a final model we need to try numerous candidate models that will be explored, examined and diagnosed. In this book we will introduce techniques that: 

* summarise how good is the current version of a model. Section \@ref(modelPerformance) overviews measures for model performance. These measures are usually used to trace the progress in model development.
* assess the feature importance. Section \@ref(featureImportance) shows how to assess influence of a single variable on model performance. Features that are not important are usually removed from a model during the model refinement. 
* shows how a single feature affects the model response. Sections \@ref(partialDependenceProfiles) -- \@ref(featureEffects) present Partial Dependency Profiles, Accumulated Local Effects and Marginal Profiles. All these techniques help to understand how model consumes particular features. 
* identifies potential problems with a model. Section \@ref(residualDiagnostic) shows techniques for exploration of model residuals. Looking closer on residuals often help to improve the model. This is possible with tools for local model exploration which are presented in the fist part of the book.
* performs sensitivity analysis for a model. Section \@ref(ceterisParibus) introduces Ceteris Paribus profiles that helps in a what-if analysis for a model.
* validated local fit for a model. Section \@ref(localDiagnostics) introduces techniques for assessment if for a single observation the model support its prediction
* decompose model predictions into pieces that can be attributed to particular variables.  Sections \@ref(breakDown) -- \@ref(LIME) show different techniques like SHAP, LIME or Break Down for local exploration of a model.


