```{r load_models_ALE, warning=FALSE, message=FALSE, echo=FALSE}
source("models/models_titanic.R")
source("models/models_apartments.R")
```

# Accumulated Local Profiles {#accumulatedLocalProfiles}

## Introduction {#ALPIntro}

One of the largest advantages of the Partial Dependency Profiles is that they are easy to explain, as they are just an average across Ceteris Paribus profiles. But one of the largest disadvantages lies in expectation over marginal distribution which implies that $x^j$ is independent from $x^{-j}$. In many applications this assumption is violated. For example, for the `apartments` dataset one can expect that features like `surface` and `number.of.rooms` are strongly correlated as apartments with larger number of rooms usually have larger surface. It may makes no sense to consider an apartment with 10 rooms and 20 square meters, so it may be misleading to change $x^{surface}$ independently from $x^{number.of.rooms}$. In the `titanic` dataset we shall expect correlation between `fare` and `passenger class` as tickets in the 1st class are the most expensive. 

There are several attempts to fix this problem. In this chapter we present two of them. Local Dependency Profiles and Accumulated Local Profiles, both introduces in the  [@ALEPlotRPackage].
The general idea behind Local Dependency Profiles is to use conditional distribution instead of marginal distribution to accommodate for the dependency between $x^j$ and $x^{-j}$.
The general idea behind Accumulated Local Profiles is to accumulate local changes in model response affected by single feature $x^j$.




## Intuition {#ALPIntuition}

Intuition behind Partial Dependency profiles and their extensions is presented in Figure \@ref(fig:accumulatedLocalEffects).

First, let's consider a simple model 

\begin{equation}
f(x_1, x_2) = x_1 * x_2 + x_2
(\#eq:trickyModel)
\end{equation}

Moreover, let's assume that variables $x_1$ and $x_2$ have uniform distribution $x_1, x_2 \sim U[-1,1]$ and are perfectly correlated, i.e. $x_2 = x_1$.

For this example, suppose that we have dataset with 8 points.


| i     | 1  |     2 |     3 |     4 |     5 |     6 |     7 |  8  |
|-------|----|-------|-------|-------|-------|-------|-------|-----|
| $x_1$ | -1 | -0.71 | -0.43 | -0.14 |  0.14 |  0.43 |  0.71 |  1  |
| $x_2$ | -1 | -0.71 | -0.43 | -0.14 |  0.14 |  0.43 |  0.71 |  1  |


Panel A in Figure \@ref(fig:accumulatedLocalEffects) shows ceteris paribus profiles calculated for selected 8 points.

Bottom part of the panel B shows Partial Dependency profile. It's an average from all ceteris paribus profiles (as shown in the top panel). 

The idea behind extensions of partial dependency profiles is to use not all profiles, but only parts that are relevant (as shown in the top panels). 
Local Dependency Profiles (panel C) are calculated as averages from these selected relevant parts of ceteris paribus profiles.
Accumulated Local Profiles (panel D) are calculated as accumulated changes from these selected relevant parts of ceteris paribus profiles.


```{r accumulatedLocalEffects, echo=FALSE, fig.cap="(fig:accumulatedLocalEffects) Differences between Partial Dependency, Marginal and Accumulated Local Effects profiles. Panel A) shows Ceteris Paribus Profiles for 8 points. Panel B) shows Partial Dependency profiles, i.e. an average out of these profiles. Panel C shows Marginal profiles, i.e. an average from profiles similar to the point that is being explained. Panel D shows Accumulated Local Effects, i.e. effect curve that takes into account only changes in the Ceteris Paribus Profiles.", out.width = '90%', fig.align='center'}
knitr::include_graphics("figure/CP_ALL.png")
```


For example, for the `apartments` dataset one can expect that features like `surface` and `number.of.rooms` are correlated but we can also imagine that each of these variables affect the apartment price somehow. Partial Dependency Profiles show how the average price changes as a function of surface, keeping all other variables unchanged. Conditional Dependency Profiles show how the average price changes as a function of surface adjusting all other variables to the current value of the surface. Accumulated Local Profiles show how the average price changes as a function of surface adjusting all other variables to the current value of the surface but extracting changes caused by these other features. 



## Method {#ALPMethod}


### Partial Dependency Profile

Partial Dependency Profile is defined as an expected value from Ceteris Paribus Profiles.

\begin{equation}
g^{PD}_i(z) = E_{X_{-i}}[ f(x|^i = z, x^{-i}) ].
(\#eq:PDPdef)
\end{equation}

And can be estimated as average from CP profiles.

\begin{equation}
\hat g^{PD}_i(z) = \frac{1}{n} \sum_{j=1}^{n} f(x|^i = z, x_j^{-i}).
(\#eq:PDPest)
\end{equation}

As it is shown in Figure \@ref(fig:accumulatedLocalEffects) panel B, PD profiles are averages from all CP profiles.

### Conditional Dependency Profile 

As it was said, if there is some dependency between $X_i$ and $X_{-i}$ it makes no sense to average CP profiles over marginal $X_{-i}$ bacuse ,,all other things kep unchanges'' is not a reliable approach. 
Instead, an intuitive approach would to use a conditional distribution $X_{-i}|X_i=x_i$ (which is of course unknown). 

Conditional Dependency Profile for a model $f$ and a variable $x^j$ is defined as

\begin{equation}
g^{CD}_{f, i}(z) = E_{X_{-i}|X_i=x_i}[ f(x|^i = z, x^{-i}) ].
(\#eq:CDPdef)
\end{equation}

So it's an expected value over **conditional** distribution $(X^j,X^{-j})|X^j=z$.

For example, let $f(x_1, x_2) = x_1 + x_2$ and distribution of $(x_1, x_2)$ is given by $x_1 \sim U[0,1]$ and $x_2=x_1$. In this case $g^{CD}_{f, 1}(z) = 2*z$.


The natural estimator for Conditional Dependency Profiles introduced in [@ALEPlotRPackage] is

\begin{equation}
\hat g^{CD}_i(z) = \frac{1}{|N_i|} \sum_{j\in N_i} f(x|^i = z, x_j^{-i}). 
(\#eq:CDPest)
\end{equation}

where $N_i$ is the set of observations with $x_i$ close to $z$. This set will be used to estimate distribution $X_{-i}|X_i=x_i$.

In Figure \@ref(fig:accumulatedLocalEffects) panel C the range of variable $x_i$ is divided into 4 separable intervals. The set $N_i$ contains all observations that fall into the same interval as observation $x_i$. The final CD profile is an average from closest pieces of CP profiles.



Note that in general the $\hat g^{CD}_i(z)$ is neither smooth, nor continuous in boundaries between $N_i$ subsets. Thus here we propose another smooth estimator for $g_i^{CD}$

\begin{equation}
\tilde g^{CD}_i(z) = \frac{1}{\sum_k w_{k}(z)} \sum_{j = 1}^n w_j(z) f(x|^i = z, x_j^{-i}) . 
(\#eq:CDPest2)
\end{equation}


Weights $w_j(z)$ correspond to the distance between $z$ and observation $x_j$. For categorical variables we may use simple indicator function $w_j(z) := 1_{z == x^i_j}$ while for continuous variables we may use Gaussian kernel
$$
w_j(z) = \phi(z - x_j^i; 0; s),
$$
where $s$ is a smoothing factor.



### Accumulated Local Profile

Accumulated Local Profile for a model $f$ and a variable $x^j$ is defined as

\begin{equation}
g^{ALE}_{f, j}(z) = \int_{z_0}^z E\left[\frac{\partial f(X^j, X^{-j})}{\partial x_j}|X^j = v\right] dv + c,
(\#eq:ALEdef)
\end{equation}

where $z_0$ if the lower boundary  of $x^j$. The profile $g^{ALE}_{f, j}(z)$ is calculated up to some constant $c$. Usually the constant $c$ is selected to set average $g^{ALE}_{f, j}$ equal to 0 or an average of $f(x)$.

The equation may be a bit complex, but the intuition is not that complicated. Instead of averaging Ceteris Paribus profiles we just look locally how quickly local CP profiles are changing. And ALE profile is reconstructed from such local partial changes as cumulative derivative over changes.

So it's a cumulated  expected change of the model response along where the expected values are calculated  over **conditional** distribution $(X^j,X^{-j})|X^j=z$.

For example, let $f(x_1, x_2) = x_1 + x_2$ and distribution  of $(x_1, x_2)$ is given by $x_1 \sim U[0,1]$ and $x_2=x_1$. In this case  $g^{ALE}_{f, 1}(z) = z$.


The natural estimator for Accumulated Local Profiles introduced in [@ALEPlotRPackage] is

\begin{equation}
\hat g^{ALE}_i(z) = \sum_{k=1}^{k_i(z)} \frac{1}{|N_i(k)|} \sum_{j \in N_i} \left[ f(x|^i = z_k) - f(x|^i = z_{k-1}) \right] + c
(\#eq:ALEPest)
\end{equation}

where $k_i(z)$ is the index of interval with point $z$, $N_i$ is the set of observations with $x_i$ in this interval. The difference $f(x|^i = z_k) - f(x|^i = z_{k-1})$ correspond to the difference of CP profiles in interval $k$, and this difference is averaged and accumulated.

In Figure \@ref(fig:accumulatedLocalEffects) panel D the range of variable $x_i$ is divided into 4 separable intervals. The set $N_i$ contains all observations that fall into the same interval as observation $x_i$. The final ALE profile is constructed from accumulated differences of local CP profiles.


Note that in general the $\hat g^{CD}_i(z)$ is not smooth in boundaries between $N_i$ subsets. Thus here we propose another smooth estimator for $g_i^{ALE}$

\begin{equation}
\tilde g^{ALE}_i(z) = \sum_{k \in {z_0, ..., z}} \frac{1}{\sum_{j} w_{j}(k)} \sum_{j=1}^n w_{j}(k) \left[f(x|^i = z_k) - f(x|^i = z_k - \Delta)\right] + c
(\#eq:ALEPest2)
\end{equation}

The set ${z_0, ..., z}$ is a uniform grid of points between $z_0$ and $z$ with the step $\Delta$.
Weights $w_i(k)$ correspond to the distance between point $k$ and observation $x_i$. For categorical variables we may use simple indicator function $w_j(k) := 1_{k == x_j^i}$ while for continuous variables we may use Gaussian kernel
$$
w_j(k) = \phi(k - x_j^i; 0; s),
$$
where $s$ is a smoothing factor.



### Comparison of Explainers for Feature Effects {#summaryFeatureEffects}

In previous sections we introduced different was to calculate model level explainers for feature effects. 
A natural question is how these approaches are different and which one should we choose.

An example that illustrate differences between these approaches is presented in Figure \@ref(fig:accumulatedLocalEffects).
Here we have a model $f(x_1, x_2) = x_1*x_2 + x_2$ and what is important features are correlated $x_1 \sim U[-1,1]$ and $x_2 = x_1$.


Panel A) shows Ceteris Paribus for 8 data points, the feature $x_1$ is on the OX axis while $f$ is on the OY. 
Panel B) shows Partial Dependency Profiles calculated as an average from CP profiles.

$$
g^{PD}_{f,1}(z) = E[z*x^2 + x^2] = 0
$$

Panel C) shows Conditional Dependency Profiles calculated as an average from conditional CP profiles. In the figure the conditioning is calculated in four bins, but knowing the formula for $f$ we can calculated it directly as.

$$
g^{CD}_{f,1}(z) = E[X^1*X^2 + X^2 | X^1 = z] = z^2+z
$$

Panel D) shows Accumulated Local Effects calculated as accumulated changes in conditional CP profiles. In the figure the conditioning is calculated in four bins, but knowing the formula for $f$ we can calculated it directly as.

$$
g^{AL}_{f,1}(z) = \int_{z_0}^z E\left[\frac{\partial (X^1*X^2 + X^2)}{\partial x_1}|X^1 = v\right] dv  = \int_{z_0}^z E\left[X^2|X^1 = v\right] dv  = \frac{z^2 -1 }{2},
$$



## Example: Apartments data {#CDPExample}


In this section we will use random forest model `apartments_rf_v5` trained on `apartments` data in order to predict the price per square meter of an apartment. See section \@ref(model-Apartments-rf) for more details.
This example is focused on two dependent variables `surface` and `no.rooms`. What is more important is that these two variables are correlated.


Figure \@ref(fig:featureEffectsApartment) shows Partial Dependency, Conditional Dependency and Accumulated Local profiles for the random forest model. 

Number of rooms and surface are two correlated variables, moreover both have some effect on the price per square meter. As we see profiles calculated with different methods are different. One we take into account the correlation between variables, the feature effects are less steep.


```{r featureEffectsApartment, warning=FALSE, message=FALSE, echo=FALSE, fig.width=8, fig.height=5.5, fig.cap="Partial Dependency, Conditional Dependency and Accumulated Local profiles for the random forest model and apartments data.", fig.align='center', out.width='75%'}
library("ingredients")
explain_apartments_rf <- explain(model_apartments_rf, 
                                 data = apartments,
                                 verbose = FALSE)

pd_rf <- partial_dependency(explain_apartments_rf, variables = c("no.rooms", "surface"))
ac_rf <- accumulated_dependency(explain_apartments_rf, variables = c("no.rooms", "surface"))
cd_rf <- conditional_dependency(explain_apartments_rf, variables = c("no.rooms", "surface"))

pd_rf$`_label_` <- "RF partial dependency"
ac_rf$`_label_` <- "RF accumulated dependency"
ac_rf$`_yhat_`  <- ac_rf$`_yhat_` + max(pd_rf$`_yhat_`)
cd_rf$`_label_` <- "RF conditional dependency"

plot(pd_rf, ac_rf, cd_rf) + ylab("feature effect") +
  ggtitle("Surface and construction year", "How they affect the expected price per square meter") 
```







## Pros and cons {#ALPProsCons}

In this chapter we introduced tools for extraction of the information between model response and individual model inputs. These tools are useful to summarize how ,,in general'' model responds to the input of interest. All presented approaches are based on Ceteris Paribus Profiles introduced in Chapter \@ref(ceterisParibus) but they differ in a way how individual profiles are merged into a global model response.

We use the term ,,feature effect'' to refer to global model response as a function of single or small number of model features. 
Methods presented in this chapter are useful for extraction information of feature effect, i.e. how a feature is linked with model response. There are many possible applications of such methods, for example:

* Feature effect may be used for feature engineering. The crude approach to modeling is to fit some elastic model on raw data and then use feature effects to understand the relation between a raw feature and model output and then to transform model input to better fit the model output. Such procedure is called surrogate training. In this procedure an elastic model is trained to learn about link between a feature and the target. Then a new feature is created in a way to better utilized the feature in a simpler model [@SAFE-arxiv]. In the next chapters we will show how feature effects can be used to transform a continuous variable in to a categorical one in order to improve the model behavior.
* Feature effect may be used for model validation.  Understanding how a model utilizes a feature  may be used as a validation of a model against domain knowledge. For example if we expect monotonic relation or linear relation then such expectations can be verified. Also if we expect smooth relation between model and its inputs then the smoothness can be visually examined. In the next chapters we will show how feature effects can be used to warn a model developer that model is unstable and should be regularized.
*  In new domains an understanding of a link between model output and the feature of interest may increase our domain knowledge. It may give quick insights related to the strength or character of the relation between a feature of interest and the model output. 
* The comparison of feature effects between different models may help to understand how different models handle particular features. In the next chapters we will show how feature effects can be used learn limitations of particular classes of models.



## Code snippets for R {#ALPR}

Here we show partial dependency profiles calculated with `ingredients` package [@ingredientsRPackage]. You will also find similar functions in the `ALEPlots` package [@ALEPlotRPackage].


Partial dependency profiles can be calculated with the function `ingredients::partial_dependency`.
Conditional dependency profiles can be calculated with the function `ingredients::conditional_dependency`.
Accumulated local profiles can be calculated with the function `ingredients::accumulated_dependency`.

In all these cases the only required argument is the explainer and by default profiles are calculated for all variables. 

Below we use `variables` argument to limit list of variables for which profiles are calculated. Here we need profiles only for the `no.rooms` and `surface` variables.


```{r aleExample1, warning=FALSE, message=FALSE, fig.width=6.5, fig.height=5,  fig.cap="Partial Dependency profile for surface and number of rooms", fig.align='center', out.width='80%'}
explain_apartments_rf <- explain(model_apartments_rf, 
                                 data = apartments,
                                 verbose = FALSE)

pd_rf <- partial_dependency(explain_apartments_rf, variables = c("no.rooms", "surface"))

plot(pd_rf) + ylab("Partial dependency") +
  ggtitle("Surface and number of rooms", "Partial dependency for random forest model") 
```

```{r aleExample2, warning=FALSE, message=FALSE, fig.width=6.5, fig.height=5,  fig.cap="Accumulated dependency profile for surface and number of rooms", fig.align='center', out.width='80%'}
ac_rf <- accumulated_dependency(explain_apartments_rf, variables = c("no.rooms", "surface"))

plot(ac_rf) + ylab("Accumulated dependency") +
  ggtitle("Surface and number of rooms", "Accumulated dependency for random forest model") 
```

```{r aleExample3, warning=FALSE, message=FALSE, fig.width=6.5, fig.height=5,  fig.cap="Conditional dependency profile for surface and number of rooms", fig.align='center', out.width='80%'}
pd_rf <- conditional_dependency(explain_apartments_rf, variables = c("no.rooms", "surface"))

plot(pd_rf) + ylab("Conditional dependency") +
  ggtitle("Surface and number of rooms", "Conditional dependency for random forest model") 
```


